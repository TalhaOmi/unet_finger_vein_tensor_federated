{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TalhaOmi/unet_finger_vein_tensor_federated/blob/main/unet_with_tensor(federated).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "EYOEDAYBBKYz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ec3d063-1139-4b0c-e346-9ce3cc58c3bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Z54AELLJA552"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETrwKfZrIT7x"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "gnxbdJ1SIVhO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25cc01c8-ee60-46d7-d36b-319ec10ce70c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"UNET\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 512, 512, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv2d_38 (Conv2D)             (None, 512, 512, 64  1792        ['input_3[0][0]']                \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_36 (BatchN  (None, 512, 512, 64  256        ['conv2d_38[0][0]']              \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " activation_36 (Activation)     (None, 512, 512, 64  0           ['batch_normalization_36[0][0]'] \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_39 (Conv2D)             (None, 512, 512, 64  36928       ['activation_36[0][0]']          \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_37 (BatchN  (None, 512, 512, 64  256        ['conv2d_39[0][0]']              \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " activation_37 (Activation)     (None, 512, 512, 64  0           ['batch_normalization_37[0][0]'] \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " max_pooling2d_8 (MaxPooling2D)  (None, 256, 256, 64  0          ['activation_37[0][0]']          \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_40 (Conv2D)             (None, 256, 256, 12  73856       ['max_pooling2d_8[0][0]']        \n",
            "                                8)                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_38 (BatchN  (None, 256, 256, 12  512        ['conv2d_40[0][0]']              \n",
            " ormalization)                  8)                                                                \n",
            "                                                                                                  \n",
            " activation_38 (Activation)     (None, 256, 256, 12  0           ['batch_normalization_38[0][0]'] \n",
            "                                8)                                                                \n",
            "                                                                                                  \n",
            " conv2d_41 (Conv2D)             (None, 256, 256, 12  147584      ['activation_38[0][0]']          \n",
            "                                8)                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_39 (BatchN  (None, 256, 256, 12  512        ['conv2d_41[0][0]']              \n",
            " ormalization)                  8)                                                                \n",
            "                                                                                                  \n",
            " activation_39 (Activation)     (None, 256, 256, 12  0           ['batch_normalization_39[0][0]'] \n",
            "                                8)                                                                \n",
            "                                                                                                  \n",
            " max_pooling2d_9 (MaxPooling2D)  (None, 128, 128, 12  0          ['activation_39[0][0]']          \n",
            "                                8)                                                                \n",
            "                                                                                                  \n",
            " conv2d_42 (Conv2D)             (None, 128, 128, 25  295168      ['max_pooling2d_9[0][0]']        \n",
            "                                6)                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_40 (BatchN  (None, 128, 128, 25  1024       ['conv2d_42[0][0]']              \n",
            " ormalization)                  6)                                                                \n",
            "                                                                                                  \n",
            " activation_40 (Activation)     (None, 128, 128, 25  0           ['batch_normalization_40[0][0]'] \n",
            "                                6)                                                                \n",
            "                                                                                                  \n",
            " conv2d_43 (Conv2D)             (None, 128, 128, 25  590080      ['activation_40[0][0]']          \n",
            "                                6)                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_41 (BatchN  (None, 128, 128, 25  1024       ['conv2d_43[0][0]']              \n",
            " ormalization)                  6)                                                                \n",
            "                                                                                                  \n",
            " activation_41 (Activation)     (None, 128, 128, 25  0           ['batch_normalization_41[0][0]'] \n",
            "                                6)                                                                \n",
            "                                                                                                  \n",
            " max_pooling2d_10 (MaxPooling2D  (None, 64, 64, 256)  0          ['activation_41[0][0]']          \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_44 (Conv2D)             (None, 64, 64, 512)  1180160     ['max_pooling2d_10[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_42 (BatchN  (None, 64, 64, 512)  2048       ['conv2d_44[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_42 (Activation)     (None, 64, 64, 512)  0           ['batch_normalization_42[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_45 (Conv2D)             (None, 64, 64, 512)  2359808     ['activation_42[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_43 (BatchN  (None, 64, 64, 512)  2048       ['conv2d_45[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_43 (Activation)     (None, 64, 64, 512)  0           ['batch_normalization_43[0][0]'] \n",
            "                                                                                                  \n",
            " max_pooling2d_11 (MaxPooling2D  (None, 32, 32, 512)  0          ['activation_43[0][0]']          \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_46 (Conv2D)             (None, 32, 32, 1024  4719616     ['max_pooling2d_11[0][0]']       \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_44 (BatchN  (None, 32, 32, 1024  4096       ['conv2d_46[0][0]']              \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " activation_44 (Activation)     (None, 32, 32, 1024  0           ['batch_normalization_44[0][0]'] \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_47 (Conv2D)             (None, 32, 32, 1024  9438208     ['activation_44[0][0]']          \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_45 (BatchN  (None, 32, 32, 1024  4096       ['conv2d_47[0][0]']              \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " activation_45 (Activation)     (None, 32, 32, 1024  0           ['batch_normalization_45[0][0]'] \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_transpose_8 (Conv2DTran  (None, 64, 64, 512)  2097664    ['activation_45[0][0]']          \n",
            " spose)                                                                                           \n",
            "                                                                                                  \n",
            " concatenate_8 (Concatenate)    (None, 64, 64, 1024  0           ['conv2d_transpose_8[0][0]',     \n",
            "                                )                                 'activation_43[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_48 (Conv2D)             (None, 64, 64, 512)  4719104     ['concatenate_8[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_46 (BatchN  (None, 64, 64, 512)  2048       ['conv2d_48[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_46 (Activation)     (None, 64, 64, 512)  0           ['batch_normalization_46[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_49 (Conv2D)             (None, 64, 64, 512)  2359808     ['activation_46[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_47 (BatchN  (None, 64, 64, 512)  2048       ['conv2d_49[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_47 (Activation)     (None, 64, 64, 512)  0           ['batch_normalization_47[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_transpose_9 (Conv2DTran  (None, 128, 128, 25  524544     ['activation_47[0][0]']          \n",
            " spose)                         6)                                                                \n",
            "                                                                                                  \n",
            " concatenate_9 (Concatenate)    (None, 128, 128, 51  0           ['conv2d_transpose_9[0][0]',     \n",
            "                                2)                                'activation_41[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_50 (Conv2D)             (None, 128, 128, 25  1179904     ['concatenate_9[0][0]']          \n",
            "                                6)                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_48 (BatchN  (None, 128, 128, 25  1024       ['conv2d_50[0][0]']              \n",
            " ormalization)                  6)                                                                \n",
            "                                                                                                  \n",
            " activation_48 (Activation)     (None, 128, 128, 25  0           ['batch_normalization_48[0][0]'] \n",
            "                                6)                                                                \n",
            "                                                                                                  \n",
            " conv2d_51 (Conv2D)             (None, 128, 128, 25  590080      ['activation_48[0][0]']          \n",
            "                                6)                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_49 (BatchN  (None, 128, 128, 25  1024       ['conv2d_51[0][0]']              \n",
            " ormalization)                  6)                                                                \n",
            "                                                                                                  \n",
            " activation_49 (Activation)     (None, 128, 128, 25  0           ['batch_normalization_49[0][0]'] \n",
            "                                6)                                                                \n",
            "                                                                                                  \n",
            " conv2d_transpose_10 (Conv2DTra  (None, 256, 256, 12  131200     ['activation_49[0][0]']          \n",
            " nspose)                        8)                                                                \n",
            "                                                                                                  \n",
            " concatenate_10 (Concatenate)   (None, 256, 256, 25  0           ['conv2d_transpose_10[0][0]',    \n",
            "                                6)                                'activation_39[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_52 (Conv2D)             (None, 256, 256, 12  295040      ['concatenate_10[0][0]']         \n",
            "                                8)                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_50 (BatchN  (None, 256, 256, 12  512        ['conv2d_52[0][0]']              \n",
            " ormalization)                  8)                                                                \n",
            "                                                                                                  \n",
            " activation_50 (Activation)     (None, 256, 256, 12  0           ['batch_normalization_50[0][0]'] \n",
            "                                8)                                                                \n",
            "                                                                                                  \n",
            " conv2d_53 (Conv2D)             (None, 256, 256, 12  147584      ['activation_50[0][0]']          \n",
            "                                8)                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_51 (BatchN  (None, 256, 256, 12  512        ['conv2d_53[0][0]']              \n",
            " ormalization)                  8)                                                                \n",
            "                                                                                                  \n",
            " activation_51 (Activation)     (None, 256, 256, 12  0           ['batch_normalization_51[0][0]'] \n",
            "                                8)                                                                \n",
            "                                                                                                  \n",
            " conv2d_transpose_11 (Conv2DTra  (None, 512, 512, 64  32832      ['activation_51[0][0]']          \n",
            " nspose)                        )                                                                 \n",
            "                                                                                                  \n",
            " concatenate_11 (Concatenate)   (None, 512, 512, 12  0           ['conv2d_transpose_11[0][0]',    \n",
            "                                8)                                'activation_37[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_54 (Conv2D)             (None, 512, 512, 64  73792       ['concatenate_11[0][0]']         \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_52 (BatchN  (None, 512, 512, 64  256        ['conv2d_54[0][0]']              \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " activation_52 (Activation)     (None, 512, 512, 64  0           ['batch_normalization_52[0][0]'] \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_55 (Conv2D)             (None, 512, 512, 64  36928       ['activation_52[0][0]']          \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_53 (BatchN  (None, 512, 512, 64  256        ['conv2d_55[0][0]']              \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " activation_53 (Activation)     (None, 512, 512, 64  0           ['batch_normalization_53[0][0]'] \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_56 (Conv2D)             (None, 512, 512, 1)  65          ['activation_53[0][0]']          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 31,055,297\n",
            "Trainable params: 31,043,521\n",
            "Non-trainable params: 11,776\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, Input\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def conv_block(inputs, num_filters):\n",
        "    x = Conv2D(num_filters, 3, padding=\"same\")(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "\n",
        "    x = Conv2D(num_filters, 3, padding=\"same\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "def encoder_block(inputs, num_filters):\n",
        "    x = conv_block(inputs, num_filters)\n",
        "    p = MaxPool2D((2, 2))(x)\n",
        "    return x, p\n",
        "\n",
        "def decoder_block(inputs, skip_features, num_filters):\n",
        "    x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(inputs)\n",
        "    x = Concatenate()([x, skip_features])\n",
        "    x = conv_block(x, num_filters)\n",
        "    return x\n",
        "\n",
        "def build_unet_main(input_shape):\n",
        "    inputs = Input(input_shape)\n",
        "\n",
        "    s1, p1 = encoder_block(inputs, 64)\n",
        "    s2, p2 = encoder_block(p1, 128)\n",
        "    s3, p3 = encoder_block(p2, 256)\n",
        "    s4, p4 = encoder_block(p3, 512)\n",
        "\n",
        "    b1 = conv_block(p4, 1024)\n",
        "\n",
        "    d1 = decoder_block(b1, s4, 512)\n",
        "    d2 = decoder_block(d1, s3, 256)\n",
        "    d3 = decoder_block(d2, s2, 128)\n",
        "    d4 = decoder_block(d3, s1, 64)\n",
        "\n",
        "    outputs = Conv2D(1, 1, padding=\"same\", activation=\"sigmoid\")(d4)\n",
        "\n",
        "    model = Model(inputs, outputs, name=\"UNET\")\n",
        "    return model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  input_shape = (512, 512, 3)\n",
        "  model_main = build_unet_main(input_shape)\n",
        "  model_main.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AH9oSmNSJz03"
      },
      "source": [
        "# Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "EF8ki0NdIWF4"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "def iou(y_true, y_pred):\n",
        "    def f(y_true, y_pred):\n",
        "        intersection = (y_true * y_pred).sum()\n",
        "        union = y_true.sum() + y_pred.sum() - intersection\n",
        "        x = (intersection + 1e-15) / (union + 1e-15)\n",
        "        x = x.astype(np.float32)\n",
        "        return x\n",
        "    return tf.numpy_function(f, [y_true, y_pred], tf.float32)\n",
        "\n",
        "smooth = 1e-15\n",
        "def dice_coef(y_true, y_pred):\n",
        "    y_true = tf.keras.layers.Flatten()(y_true)\n",
        "    y_pred = tf.keras.layers.Flatten()(y_pred)\n",
        "    intersection = tf.reduce_sum(y_true * y_pred)\n",
        "    return (2. * intersection + smooth) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + smooth)\n",
        "\n",
        "def dice_loss(y_true, y_pred):\n",
        "    return 1.0 - dice_coef(y_true, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#with open(\"/content/drive/MyDrive/Colab Notebooks/Dataset/new_data/files/test\", \"rb\") as fp:   # Unpickling\n",
        "#   b = pickle.load(fp)"
      ],
      "metadata": {
        "id": "my1HBmwb3VzE"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4wJiAdXKCbg"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "7MPRcaAgKEK0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcac12b0-8517-415b-efe7-44ead4a5926a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 19 - 19\n",
            "Valid: 5 - 5\n",
            "Epoch 1/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.9625 - dice_coef: 0.0375 - iou: 0.0192 - recall_1: 0.4115 - precision_1: 0.3359\n",
            "Epoch 1: val_loss improved from inf to 0.97905, saving model to /content/drive/MyDrive/Colab Notebooks/Dataset/new_data/files/model.h5\n",
            "19/19 [==============================] - 10s 400ms/step - loss: 0.9625 - dice_coef: 0.0375 - iou: 0.0192 - recall_1: 0.4115 - precision_1: 0.3359 - val_loss: 0.9790 - val_dice_coef: 0.0210 - val_iou: 0.0106 - val_recall_1: 1.0000 - val_precision_1: 0.4030 - lr: 1.0000e-04\n",
            "Epoch 2/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.9515 - dice_coef: 0.0485 - iou: 0.0249 - recall_1: 0.3159 - precision_1: 0.3804\n",
            "Epoch 2: val_loss improved from 0.97905 to 0.97903, saving model to /content/drive/MyDrive/Colab Notebooks/Dataset/new_data/files/model.h5\n",
            "19/19 [==============================] - 7s 368ms/step - loss: 0.9515 - dice_coef: 0.0485 - iou: 0.0249 - recall_1: 0.3159 - precision_1: 0.3804 - val_loss: 0.9790 - val_dice_coef: 0.0210 - val_iou: 0.0106 - val_recall_1: 1.0000 - val_precision_1: 0.4030 - lr: 1.0000e-04\n",
            "Epoch 3/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.9395 - dice_coef: 0.0605 - iou: 0.0312 - recall_1: 0.1967 - precision_1: 0.4195\n",
            "Epoch 3: val_loss did not improve from 0.97903\n",
            "19/19 [==============================] - 5s 286ms/step - loss: 0.9395 - dice_coef: 0.0605 - iou: 0.0312 - recall_1: 0.1967 - precision_1: 0.4195 - val_loss: 0.9790 - val_dice_coef: 0.0210 - val_iou: 0.0106 - val_recall_1: 0.9998 - val_precision_1: 0.4029 - lr: 1.0000e-04\n",
            "Epoch 4/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.9276 - dice_coef: 0.0724 - iou: 0.0376 - recall_1: 0.1573 - precision_1: 0.4707\n",
            "Epoch 4: val_loss did not improve from 0.97903\n",
            "19/19 [==============================] - 6s 299ms/step - loss: 0.9276 - dice_coef: 0.0724 - iou: 0.0376 - recall_1: 0.1573 - precision_1: 0.4707 - val_loss: 0.9791 - val_dice_coef: 0.0209 - val_iou: 0.0106 - val_recall_1: 0.9606 - val_precision_1: 0.4013 - lr: 1.0000e-04\n",
            "Epoch 5/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.9204 - dice_coef: 0.0796 - iou: 0.0415 - recall_1: 0.1461 - precision_1: 0.4966\n",
            "Epoch 5: val_loss did not improve from 0.97903\n",
            "19/19 [==============================] - 5s 289ms/step - loss: 0.9204 - dice_coef: 0.0796 - iou: 0.0415 - recall_1: 0.1461 - precision_1: 0.4966 - val_loss: 0.9791 - val_dice_coef: 0.0209 - val_iou: 0.0105 - val_recall_1: 0.1177 - val_precision_1: 0.2551 - lr: 1.0000e-04\n",
            "Epoch 6/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.9164 - dice_coef: 0.0836 - iou: 0.0436 - recall_1: 0.1308 - precision_1: 0.5097\n",
            "Epoch 6: val_loss improved from 0.97903 to 0.97888, saving model to /content/drive/MyDrive/Colab Notebooks/Dataset/new_data/files/model.h5\n",
            "19/19 [==============================] - 11s 600ms/step - loss: 0.9164 - dice_coef: 0.0836 - iou: 0.0436 - recall_1: 0.1308 - precision_1: 0.5097 - val_loss: 0.9789 - val_dice_coef: 0.0211 - val_iou: 0.0107 - val_recall_1: 0.0033 - val_precision_1: 0.2973 - lr: 1.0000e-04\n",
            "Epoch 7/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.9081 - dice_coef: 0.0919 - iou: 0.0482 - recall_1: 0.1274 - precision_1: 0.5563\n",
            "Epoch 7: val_loss improved from 0.97888 to 0.97843, saving model to /content/drive/MyDrive/Colab Notebooks/Dataset/new_data/files/model.h5\n",
            "19/19 [==============================] - 10s 522ms/step - loss: 0.9081 - dice_coef: 0.0919 - iou: 0.0482 - recall_1: 0.1274 - precision_1: 0.5563 - val_loss: 0.9784 - val_dice_coef: 0.0216 - val_iou: 0.0109 - val_recall_1: 5.6799e-05 - val_precision_1: 0.2459 - lr: 1.0000e-04\n",
            "Epoch 8/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.9021 - dice_coef: 0.0979 - iou: 0.0515 - recall_1: 0.1178 - precision_1: 0.5635\n",
            "Epoch 8: val_loss did not improve from 0.97843\n",
            "19/19 [==============================] - 6s 290ms/step - loss: 0.9021 - dice_coef: 0.0979 - iou: 0.0515 - recall_1: 0.1178 - precision_1: 0.5635 - val_loss: 0.9786 - val_dice_coef: 0.0214 - val_iou: 0.0108 - val_recall_1: 1.8933e-06 - val_precision_1: 0.5000 - lr: 1.0000e-04\n",
            "Epoch 9/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.8953 - dice_coef: 0.1047 - iou: 0.0553 - recall_1: 0.1144 - precision_1: 0.5874\n",
            "Epoch 9: val_loss improved from 0.97843 to 0.97840, saving model to /content/drive/MyDrive/Colab Notebooks/Dataset/new_data/files/model.h5\n",
            "19/19 [==============================] - 10s 532ms/step - loss: 0.8953 - dice_coef: 0.1047 - iou: 0.0553 - recall_1: 0.1144 - precision_1: 0.5874 - val_loss: 0.9784 - val_dice_coef: 0.0216 - val_iou: 0.0109 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00 - lr: 1.0000e-04\n",
            "Epoch 10/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.8913 - dice_coef: 0.1087 - iou: 0.0575 - recall_1: 0.1060 - precision_1: 0.6152\n",
            "Epoch 10: val_loss did not improve from 0.97840\n",
            "19/19 [==============================] - 6s 291ms/step - loss: 0.8913 - dice_coef: 0.1087 - iou: 0.0575 - recall_1: 0.1060 - precision_1: 0.6152 - val_loss: 0.9790 - val_dice_coef: 0.0210 - val_iou: 0.0106 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00 - lr: 1.0000e-04\n",
            "Epoch 11/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.8877 - dice_coef: 0.1123 - iou: 0.0595 - recall_1: 0.1099 - precision_1: 0.6157\n",
            "Epoch 11: val_loss did not improve from 0.97840\n",
            "19/19 [==============================] - 6s 292ms/step - loss: 0.8877 - dice_coef: 0.1123 - iou: 0.0595 - recall_1: 0.1099 - precision_1: 0.6157 - val_loss: 0.9792 - val_dice_coef: 0.0208 - val_iou: 0.0105 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00 - lr: 1.0000e-04\n",
            "Epoch 12/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.8843 - dice_coef: 0.1157 - iou: 0.0615 - recall_1: 0.1008 - precision_1: 0.6397\n",
            "Epoch 12: val_loss did not improve from 0.97840\n",
            "\n",
            "Epoch 12: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
            "19/19 [==============================] - 6s 305ms/step - loss: 0.8843 - dice_coef: 0.1157 - iou: 0.0615 - recall_1: 0.1008 - precision_1: 0.6397 - val_loss: 0.9793 - val_dice_coef: 0.0207 - val_iou: 0.0104 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00 - lr: 1.0000e-04\n",
            "Epoch 13/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.8771 - dice_coef: 0.1229 - iou: 0.0655 - recall_1: 0.1068 - precision_1: 0.6478\n",
            "Epoch 13: val_loss improved from 0.97840 to 0.97836, saving model to /content/drive/MyDrive/Colab Notebooks/Dataset/new_data/files/model.h5\n",
            "19/19 [==============================] - 10s 520ms/step - loss: 0.8771 - dice_coef: 0.1229 - iou: 0.0655 - recall_1: 0.1068 - precision_1: 0.6478 - val_loss: 0.9784 - val_dice_coef: 0.0216 - val_iou: 0.0109 - val_recall_1: 5.5852e-04 - val_precision_1: 1.0000 - lr: 1.0000e-05\n",
            "Epoch 14/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.8695 - dice_coef: 0.1305 - iou: 0.0698 - recall_1: 0.1015 - precision_1: 0.6798\n",
            "Epoch 14: val_loss improved from 0.97836 to 0.97786, saving model to /content/drive/MyDrive/Colab Notebooks/Dataset/new_data/files/model.h5\n",
            "19/19 [==============================] - 8s 418ms/step - loss: 0.8695 - dice_coef: 0.1305 - iou: 0.0698 - recall_1: 0.1015 - precision_1: 0.6798 - val_loss: 0.9779 - val_dice_coef: 0.0221 - val_iou: 0.0112 - val_recall_1: 0.0012 - val_precision_1: 1.0000 - lr: 1.0000e-05\n",
            "Epoch 15/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.8654 - dice_coef: 0.1346 - iou: 0.0722 - recall_1: 0.0992 - precision_1: 0.6904\n",
            "Epoch 15: val_loss did not improve from 0.97786\n",
            "19/19 [==============================] - 6s 296ms/step - loss: 0.8654 - dice_coef: 0.1346 - iou: 0.0722 - recall_1: 0.0992 - precision_1: 0.6904 - val_loss: 0.9781 - val_dice_coef: 0.0219 - val_iou: 0.0111 - val_recall_1: 0.0013 - val_precision_1: 1.0000 - lr: 1.0000e-05\n",
            "Epoch 16/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.8627 - dice_coef: 0.1373 - iou: 0.0737 - recall_1: 0.0964 - precision_1: 0.7026\n",
            "Epoch 16: val_loss improved from 0.97786 to 0.97764, saving model to /content/drive/MyDrive/Colab Notebooks/Dataset/new_data/files/model.h5\n",
            "19/19 [==============================] - 12s 627ms/step - loss: 0.8627 - dice_coef: 0.1373 - iou: 0.0737 - recall_1: 0.0964 - precision_1: 0.7026 - val_loss: 0.9776 - val_dice_coef: 0.0224 - val_iou: 0.0113 - val_recall_1: 0.0017 - val_precision_1: 1.0000 - lr: 1.0000e-05\n",
            "Epoch 17/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.8602 - dice_coef: 0.1398 - iou: 0.0752 - recall_1: 0.0944 - precision_1: 0.7121\n",
            "Epoch 17: val_loss did not improve from 0.97764\n",
            "19/19 [==============================] - 6s 307ms/step - loss: 0.8602 - dice_coef: 0.1398 - iou: 0.0752 - recall_1: 0.0944 - precision_1: 0.7121 - val_loss: 0.9781 - val_dice_coef: 0.0219 - val_iou: 0.0111 - val_recall_1: 0.0016 - val_precision_1: 1.0000 - lr: 1.0000e-05\n",
            "Epoch 18/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.8583 - dice_coef: 0.1417 - iou: 0.0763 - recall_1: 0.0931 - precision_1: 0.7174\n",
            "Epoch 18: val_loss did not improve from 0.97764\n",
            "19/19 [==============================] - 6s 310ms/step - loss: 0.8583 - dice_coef: 0.1417 - iou: 0.0763 - recall_1: 0.0931 - precision_1: 0.7174 - val_loss: 0.9780 - val_dice_coef: 0.0220 - val_iou: 0.0111 - val_recall_1: 0.0018 - val_precision_1: 1.0000 - lr: 1.0000e-05\n",
            "Epoch 19/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.8562 - dice_coef: 0.1438 - iou: 0.0775 - recall_1: 0.0917 - precision_1: 0.7254\n",
            "Epoch 19: val_loss did not improve from 0.97764\n",
            "19/19 [==============================] - 6s 296ms/step - loss: 0.8562 - dice_coef: 0.1438 - iou: 0.0775 - recall_1: 0.0917 - precision_1: 0.7254 - val_loss: 0.9781 - val_dice_coef: 0.0219 - val_iou: 0.0111 - val_recall_1: 0.0019 - val_precision_1: 1.0000 - lr: 1.0000e-05\n",
            "Epoch 20/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.8543 - dice_coef: 0.1457 - iou: 0.0786 - recall_1: 0.0910 - precision_1: 0.7299\n",
            "Epoch 20: val_loss did not improve from 0.97764\n",
            "19/19 [==============================] - 6s 297ms/step - loss: 0.8543 - dice_coef: 0.1457 - iou: 0.0786 - recall_1: 0.0910 - precision_1: 0.7299 - val_loss: 0.9777 - val_dice_coef: 0.0223 - val_iou: 0.0113 - val_recall_1: 0.0022 - val_precision_1: 0.9933 - lr: 1.0000e-05\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
        "import numpy as np\n",
        "import cv2\n",
        "from glob import glob\n",
        "from sklearn.utils import shuffle\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau, EarlyStopping, TensorBoard\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import Recall, Precision\n",
        "\n",
        "\n",
        "H = 512\n",
        "W = 512\n",
        "\n",
        "def create_dir(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "def load_data(path):\n",
        "    x = sorted(glob(os.path.join(path, \"set1\",\"image\", \"*\")))\n",
        "    y = sorted(glob(os.path.join(path, \"set1\",\"mask\", \"*\")))\n",
        "    return x, y\n",
        "\n",
        "def shuffling(x, y):\n",
        "    x, y = shuffle(x, y, random_state=42)\n",
        "    return x, y\n",
        "\n",
        "def read_image(path):\n",
        "    path = path.decode()\n",
        "    x = cv2.imread(path, cv2.IMREAD_COLOR)\n",
        "    # x = cv2.resize(x, (W, H))\n",
        "    x = x/255.0\n",
        "    x = x.astype(np.float32)\n",
        "    return x\n",
        "\n",
        "def read_mask(path):\n",
        "    path = path.decode()\n",
        "    x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)  ## (512, 512)\n",
        "    # x = cv2.resize(x, (W, H))\n",
        "    x = x/255.0\n",
        "    x = x.astype(np.float32)\n",
        "    x = np.expand_dims(x, axis=-1)              ## (512, 512, 1)\n",
        "    return x\n",
        "\n",
        "def tf_parse(x, y):\n",
        "    def _parse(x, y):\n",
        "        x = read_image(x)\n",
        "        y = read_mask(y)\n",
        "        return x, y\n",
        "\n",
        "    x, y = tf.numpy_function(_parse, [x, y], [tf.float32, tf.float32])\n",
        "    x.set_shape([H, W, 3])\n",
        "    y.set_shape([H, W, 1])\n",
        "    return x, y\n",
        "\n",
        "def tf_dataset(X, Y, batch_size=2):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X, Y))\n",
        "    dataset = dataset.map(tf_parse)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(4)\n",
        "    return dataset\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    \"\"\" Seeding \"\"\"\n",
        "    np.random.seed(42)\n",
        "    tf.random.set_seed(42)\n",
        "\n",
        "    \"\"\" Directory to save files \"\"\"\n",
        "    create_dir(\"/content/drive/MyDrive/Colab Notebooks/Dataset/new_data/files\")\n",
        "\n",
        "    \"\"\" Hyperparameters \"\"\"\n",
        "    batch_size = 1\n",
        "    lr = 1e-4\n",
        "    num_epochs = 20\n",
        "    model_path = os.path.join(\"/content/drive/MyDrive/Colab Notebooks/Dataset/new_data/files\", \"model.h5\")\n",
        "    csv_path = os.path.join(\"/content/drive/MyDrive/Colab Notebooks/Dataset/new_data/files\", \"data.csv\")\n",
        "\n",
        "    \"\"\" Dataset \"\"\"\n",
        "    dataset_path = \"/content/drive/MyDrive/Colab Notebooks/Dataset/new_data\"\n",
        "    train_path = os.path.join(dataset_path, \"train\")\n",
        "    valid_path = os.path.join(dataset_path, \"valid\")\n",
        "\n",
        "    train_x, train_y = load_data(train_path)\n",
        "    train_x, train_y = shuffling(train_x, train_y)\n",
        "    valid_x, valid_y = load_data(valid_path)\n",
        "\n",
        "    print(f\"Train: {len(train_x)} - {len(train_y)}\")\n",
        "    print(f\"Valid: {len(valid_x)} - {len(valid_y)}\")\n",
        "\n",
        "    train_dataset = tf_dataset(train_x, train_y, batch_size=batch_size)\n",
        "    valid_dataset = tf_dataset(valid_x, valid_y, batch_size=batch_size)\n",
        "\n",
        "    train_steps = len(train_x)//batch_size\n",
        "    valid_setps = len(valid_x)//batch_size\n",
        "\n",
        "    if len(train_x) % batch_size != 0:\n",
        "        train_steps += 1\n",
        "    if len(valid_x) % batch_size != 0:\n",
        "        valid_setps += 1\n",
        "\n",
        "    \"\"\" Model \"\"\"\n",
        "    model_main = build_unet_main((H, W, 3))\n",
        "    #print(model_main.get_weights())\n",
        "    #model_main.set_weights(b)\n",
        "    model_main.compile(loss=dice_loss, optimizer=Adam(lr), metrics=[dice_coef, iou, Recall(), Precision()])\n",
        "    #model.summary()\n",
        "\n",
        "    callbacks = [\n",
        "        ModelCheckpoint(model_path, verbose=1, save_best_only=True),\n",
        "        ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1, patience=5, min_lr=1e-6, verbose=1),\n",
        "        CSVLogger(csv_path),\n",
        "        TensorBoard(),\n",
        "        EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=False)\n",
        "    ]\n",
        "\n",
        "    model_main.fit(\n",
        "        train_dataset,\n",
        "        epochs=num_epochs,\n",
        "        validation_data=valid_dataset,\n",
        "        steps_per_epoch=train_steps,\n",
        "        validation_steps=valid_setps,\n",
        "        callbacks=callbacks\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "FNbIsIxVKy9a"
      },
      "outputs": [],
      "source": [
        "main_model_weight=model_main.get_weights()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cceRNgnS6Rh"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "-734GsxIS8qA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac71d6ff-c71c-464f-95de-0b888635f8d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 1/10 [00:00<00:07,  1.27it/s]/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            " 20%|██        | 2/10 [00:01<00:04,  1.76it/s]/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            " 30%|███       | 3/10 [00:01<00:03,  2.03it/s]/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            " 40%|████      | 4/10 [00:02<00:02,  2.07it/s]/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            " 50%|█████     | 5/10 [00:02<00:02,  2.19it/s]/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1580: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in labels with no true or predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            " 60%|██████    | 6/10 [00:02<00:01,  2.29it/s]/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            " 70%|███████   | 7/10 [00:03<00:01,  2.27it/s]/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1580: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in labels with no true or predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            " 80%|████████  | 8/10 [00:03<00:00,  2.25it/s]/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1580: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in labels with no true or predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            " 90%|█████████ | 9/10 [00:04<00:00,  2.30it/s]/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1580: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in labels with no true or predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "100%|██████████| 10/10 [00:04<00:00,  2.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 0.99896\n",
            "F1: 0.49974\n",
            "Jaccard: 0.49948\n",
            "Recall: 0.49948\n",
            "Precision: 0.50000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import CustomObjectScope\n",
        "from sklearn.metrics import accuracy_score, f1_score, jaccard_score, precision_score, recall_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "H = 512\n",
        "W = 512\n",
        "\n",
        "def create_dir(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "def read_image(path):\n",
        "    x = cv2.imread(path, cv2.IMREAD_COLOR)\n",
        "    # x = cv2.resize(x, (W, H))\n",
        "    ori_x = x\n",
        "    x = x/255.0\n",
        "    x = x.astype(np.float32)\n",
        "    return ori_x, x\n",
        "\n",
        "def read_mask(path):\n",
        "    y = cv2.imread(path, cv2.IMREAD_GRAYSCALE)  ## (512, 512)\n",
        "    # x = cv2.resize(x, (W, H))\n",
        "    ori_y = y\n",
        "    y = y/255.0\n",
        "    y = y.astype(np.int32)\n",
        "    return ori_y, y\n",
        "\n",
        "\n",
        "\n",
        "def save_results(ori_x, ori_y, y_pred, save_image_path):\n",
        "    line = np.ones((H, 10, 3)) * 255\n",
        "\n",
        "    ori_y = np.expand_dims(ori_y, axis=-1)\n",
        "    ori_y = np.concatenate([ori_y, ori_y, ori_y], axis=-1)\n",
        "\n",
        "    y_pred = np.expand_dims(y_pred, axis=-1)\n",
        "    y_pred = np.concatenate([y_pred, y_pred, y_pred], axis=-1) * 255\n",
        "\n",
        "    cat_images = np.concatenate([ori_x, line, ori_y, line, y_pred], axis=1)\n",
        "    cv2.imwrite(save_image_path, cat_images)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    \"\"\" Save the results in this folder \"\"\"\n",
        "    create_dir(\"/content/drive/MyDrive/Colab Notebooks/Dataset/new_data/results1\")\n",
        "\n",
        "    \"\"\" Load the model \"\"\"\n",
        "    with CustomObjectScope({'iou': iou, 'dice_coef': dice_coef, 'dice_loss': dice_loss}):\n",
        "        model = tf.keras.models.load_model(\"/content/drive/MyDrive/Colab Notebooks/Dataset/new_data/files/model.h5\")\n",
        "\n",
        "    \"\"\" Load the dataset \"\"\"\n",
        "    test_x = sorted(glob(\"/content/drive/MyDrive/Colab Notebooks/Dataset/new_data/test/image/*\"))\n",
        "    test_y = sorted(glob(\"/content/drive/MyDrive/Colab Notebooks/Dataset/new_data/test/mask/*\"))\n",
        "\n",
        "    \"\"\" Make the prediction and calculate the metrics values \"\"\"\n",
        "    SCORE = []\n",
        "    for x, y in tqdm(zip(test_x, test_y), total=len(test_x)):\n",
        "        \"\"\" Extracting name \"\"\"\n",
        "        name = x.split(\"/\")[-1].split(\".\")[0]\n",
        "\n",
        "        \"\"\" Read the image and mask \"\"\"\n",
        "        ori_x, x = read_image(x)\n",
        "        ori_y, y = read_mask(y)\n",
        "\n",
        "        \"\"\" Prediction \"\"\"\n",
        "        y_pred = model.predict(np.expand_dims(x, axis=0))[0]\n",
        "        y_pred = y_pred > 0.5\n",
        "        y_pred = y_pred.astype(np.int32)\n",
        "        y_pred = np.squeeze(y_pred, axis=-1)\n",
        "\n",
        "        \"\"\" Saving the images \"\"\"\n",
        "        save_image_path = f\"/content/drive/MyDrive/Colab Notebooks/Dataset/new_data/results1/{name}.png\"\n",
        "        save_results(ori_x, ori_y, y_pred, save_image_path)\n",
        "\n",
        "        \"\"\" Flatten the array \"\"\"\n",
        "        y = y.flatten()\n",
        "        y_pred = y_pred.flatten()\n",
        "\n",
        "        \"\"\" Calculate the metrics \"\"\"\n",
        "        acc_value = accuracy_score(y, y_pred)\n",
        "        f1_value = f1_score(y, y_pred,labels=[0, 1], average=\"macro\")\n",
        "        jac_value = jaccard_score(y, y_pred,labels=[0, 1], average=\"macro\")\n",
        "        recall_value = recall_score(y, y_pred,labels=[0, 1], average=\"macro\")\n",
        "        precision_value = precision_score(y, y_pred,labels=[0, 1], average=\"macro\")\n",
        "        SCORE.append([name, acc_value, f1_value, jac_value, recall_value, precision_value])\n",
        "\n",
        "    score = [s[1:] for s in SCORE]\n",
        "    score = np.mean(score, axis=0)\n",
        "    print()\n",
        "    print(f\"Accuracy: {score[0]:0.5f}\")\n",
        "    print(f\"F1: {score[1]:0.5f}\")\n",
        "    print(f\"Jaccard: {score[2]:0.5f}\")\n",
        "    print(f\"Recall: {score[3]:0.5f}\")\n",
        "    print(f\"Precision: {score[4]:0.5f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 1(test)"
      ],
      "metadata": {
        "id": "eg-aqusjcipU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "o71Dx2WX9EbM"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, Input\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def conv_block(inputs, num_filters):\n",
        "    x = Conv2D(num_filters, 3, padding=\"same\")(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "\n",
        "    x = Conv2D(num_filters, 3, padding=\"same\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "def encoder_block(inputs, num_filters):\n",
        "    x = conv_block(inputs, num_filters)\n",
        "    p = MaxPool2D((2, 2))(x)\n",
        "    return x, p\n",
        "\n",
        "def decoder_block(inputs, skip_features, num_filters):\n",
        "    x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(inputs)\n",
        "    x = Concatenate()([x, skip_features])\n",
        "    x = conv_block(x, num_filters)\n",
        "    return x\n",
        "\n",
        "def build_unet_1(input_shape):\n",
        "    inputs = Input(input_shape)\n",
        "\n",
        "    s1, p1 = encoder_block(inputs, 64)\n",
        "    s2, p2 = encoder_block(p1, 128)\n",
        "    s3, p3 = encoder_block(p2, 256)\n",
        "    s4, p4 = encoder_block(p3, 512)\n",
        "\n",
        "    b1 = conv_block(p4, 1024)\n",
        "\n",
        "    d1 = decoder_block(b1, s4, 512)\n",
        "    d2 = decoder_block(d1, s3, 256)\n",
        "    d3 = decoder_block(d2, s2, 128)\n",
        "    d4 = decoder_block(d3, s1, 64)\n",
        "\n",
        "    outputs = Conv2D(1, 1, padding=\"same\", activation=\"sigmoid\")(d4)\n",
        "\n",
        "    model = Model(inputs, outputs, name=\"UNET\")\n",
        "    return model\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
        "import numpy as np\n",
        "import cv2\n",
        "from glob import glob\n",
        "from sklearn.utils import shuffle\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau, EarlyStopping, TensorBoard\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import Recall, Precision\n",
        "\n",
        "\n",
        "H = 512\n",
        "W = 512\n",
        "\n",
        "def create_dir(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "def load_data(path):\n",
        "    x = sorted(glob(os.path.join(path, \"set2\",\"image\", \"*\")))\n",
        "    y = sorted(glob(os.path.join(path, \"set2\",\"mask\", \"*\")))\n",
        "    return x, y\n",
        "\n",
        "def shuffling(x, y):\n",
        "    x, y = shuffle(x, y, random_state=42)\n",
        "    return x, y\n",
        "\n",
        "def read_image(path):\n",
        "    path = path.decode()\n",
        "    x = cv2.imread(path, cv2.IMREAD_COLOR)\n",
        "    # x = cv2.resize(x, (W, H))\n",
        "    x = x/255.0\n",
        "    x = x.astype(np.float32)\n",
        "    return x\n",
        "\n",
        "def read_mask(path):\n",
        "    path = path.decode()\n",
        "    x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)  ## (512, 512)\n",
        "    # x = cv2.resize(x, (W, H))\n",
        "    x = x/255.0\n",
        "    x = x.astype(np.float32)\n",
        "    x = np.expand_dims(x, axis=-1)              ## (512, 512, 1)\n",
        "    return x\n",
        "\n",
        "def tf_parse(x, y):\n",
        "    def _parse(x, y):\n",
        "        x = read_image(x)\n",
        "        y = read_mask(y)\n",
        "        return x, y\n",
        "\n",
        "    x, y = tf.numpy_function(_parse, [x, y], [tf.float32, tf.float32])\n",
        "    x.set_shape([H, W, 3])\n",
        "    y.set_shape([H, W, 1])\n",
        "    return x, y\n",
        "\n",
        "def tf_dataset(X, Y, batch_size=2):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X, Y))\n",
        "    dataset = dataset.map(tf_parse)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(4)\n",
        "    return dataset\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    \"\"\" Seeding \"\"\"\n",
        "    np.random.seed(42)\n",
        "    tf.random.set_seed(42)\n",
        "\n",
        "    \"\"\" Directory to save files \"\"\"\n",
        "    create_dir(\"/content/drive/MyDrive/Colab Notebooks/Dataset/new_data/files\")\n",
        "\n",
        "    \"\"\" Hyperparameters \"\"\"\n",
        "    batch_size = 1\n",
        "    lr = 1e-4\n",
        "    num_epochs = 20\n",
        "    model_path = os.path.join(\"/content/drive/MyDrive/Colab Notebooks/Dataset/new_data/files\", \"model1.h5\")\n",
        "    csv_path = os.path.join(\"/content/drive/MyDrive/Colab Notebooks/Dataset/new_data/files\", \"data1.csv\")\n",
        "\n",
        "    \"\"\" Dataset \"\"\"\n",
        "    dataset_path = \"/content/drive/MyDrive/Colab Notebooks/Dataset/new_data\"\n",
        "    train_path = os.path.join(dataset_path, \"train\")\n",
        "    valid_path = os.path.join(dataset_path, \"valid\")\n",
        "\n",
        "    train_x, train_y = load_data(train_path)\n",
        "    train_x, train_y = shuffling(train_x, train_y)\n",
        "    valid_x, valid_y = load_data(valid_path)\n",
        "\n",
        "    print(f\"Train: {len(train_x)} - {len(train_y)}\")\n",
        "    print(f\"Valid: {len(valid_x)} - {len(valid_y)}\")\n",
        "\n",
        "    train_dataset = tf_dataset(train_x, train_y, batch_size=batch_size)\n",
        "    valid_dataset = tf_dataset(valid_x, valid_y, batch_size=batch_size)\n",
        "\n",
        "    train_steps = len(train_x)//batch_size\n",
        "    valid_setps = len(valid_x)//batch_size\n",
        "\n",
        "    if len(train_x) % batch_size != 0:\n",
        "        train_steps += 1\n",
        "    if len(valid_x) % batch_size != 0:\n",
        "        valid_setps += 1\n",
        "\n",
        "    \"\"\" Model \"\"\"\n",
        "    model1 = build_unet_1((H, W, 3))\n",
        "    model1.set_weights(main_model_weight)\n",
        "    model1.compile(loss=dice_loss, optimizer=Adam(lr), metrics=[dice_coef, iou, Recall(), Precision()])\n",
        "    # model.summary()\n",
        "\n",
        "    callbacks = [\n",
        "        ModelCheckpoint(model_path, verbose=1, save_best_only=True),\n",
        "        ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1, patience=5, min_lr=1e-6, verbose=1),\n",
        "        CSVLogger(csv_path),\n",
        "        TensorBoard(),\n",
        "        EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=False)\n",
        "    ]\n",
        "\n",
        "    model1.fit(\n",
        "        train_dataset,\n",
        "        epochs=num_epochs,\n",
        "        validation_data=valid_dataset,\n",
        "        steps_per_epoch=train_steps,\n",
        "        validation_steps=valid_setps,\n",
        "        callbacks=callbacks\n",
        "    )"
      ],
      "metadata": {
        "id": "8mDKJ9SFdVix",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c23571ee-c3d2-432f-f7b1-4dfb1df7ca32"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 19 - 19\n",
            "Valid: 5 - 5\n",
            "Epoch 1/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.9157 - dice_coef: 0.0843 - iou: 0.0441 - recall_2: 0.2049 - precision_2: 0.5225\n",
            "Epoch 1: val_loss improved from inf to 0.97587, saving model to /content/drive/MyDrive/Colab Notebooks/Dataset/new_data/files/model1.h5\n",
            "19/19 [==============================] - 29s 1s/step - loss: 0.9157 - dice_coef: 0.0843 - iou: 0.0441 - recall_2: 0.2049 - precision_2: 0.5225 - val_loss: 0.9759 - val_dice_coef: 0.0241 - val_iou: 0.0122 - val_recall_2: 0.1232 - val_precision_2: 0.0946 - lr: 1.0000e-04\n",
            "Epoch 2/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.9023 - dice_coef: 0.0977 - iou: 0.0514 - recall_2: 0.2054 - precision_2: 0.5752\n",
            "Epoch 2: val_loss improved from 0.97587 to 0.96789, saving model to /content/drive/MyDrive/Colab Notebooks/Dataset/new_data/files/model1.h5\n",
            "19/19 [==============================] - 7s 380ms/step - loss: 0.9023 - dice_coef: 0.0977 - iou: 0.0514 - recall_2: 0.2054 - precision_2: 0.5752 - val_loss: 0.9679 - val_dice_coef: 0.0321 - val_iou: 0.0163 - val_recall_2: 0.2374 - val_precision_2: 0.1296 - lr: 1.0000e-04\n",
            "Epoch 3/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.8919 - dice_coef: 0.1081 - iou: 0.0572 - recall_2: 0.2055 - precision_2: 0.5998\n",
            "Epoch 3: val_loss improved from 0.96789 to 0.96279, saving model to /content/drive/MyDrive/Colab Notebooks/Dataset/new_data/files/model1.h5\n",
            "19/19 [==============================] - 7s 377ms/step - loss: 0.8919 - dice_coef: 0.1081 - iou: 0.0572 - recall_2: 0.2055 - precision_2: 0.5998 - val_loss: 0.9628 - val_dice_coef: 0.0372 - val_iou: 0.0190 - val_recall_2: 0.1457 - val_precision_2: 0.2082 - lr: 1.0000e-04\n",
            "Epoch 4/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.8820 - dice_coef: 0.1180 - iou: 0.0627 - recall_2: 0.2100 - precision_2: 0.6288\n",
            "Epoch 4: val_loss improved from 0.96279 to 0.95632, saving model to /content/drive/MyDrive/Colab Notebooks/Dataset/new_data/files/model1.h5\n",
            "19/19 [==============================] - 7s 385ms/step - loss: 0.8820 - dice_coef: 0.1180 - iou: 0.0627 - recall_2: 0.2100 - precision_2: 0.6288 - val_loss: 0.9563 - val_dice_coef: 0.0437 - val_iou: 0.0224 - val_recall_2: 0.1738 - val_precision_2: 0.2622 - lr: 1.0000e-04\n",
            "Epoch 5/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.8778 - dice_coef: 0.1222 - iou: 0.0652 - recall_2: 0.1972 - precision_2: 0.6509\n",
            "Epoch 5: val_loss improved from 0.95632 to 0.95108, saving model to /content/drive/MyDrive/Colab Notebooks/Dataset/new_data/files/model1.h5\n",
            "19/19 [==============================] - 7s 382ms/step - loss: 0.8778 - dice_coef: 0.1222 - iou: 0.0652 - recall_2: 0.1972 - precision_2: 0.6509 - val_loss: 0.9511 - val_dice_coef: 0.0489 - val_iou: 0.0251 - val_recall_2: 0.1494 - val_precision_2: 0.3348 - lr: 1.0000e-04\n",
            "Epoch 6/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.8710 - dice_coef: 0.1290 - iou: 0.0690 - recall_2: 0.2020 - precision_2: 0.6446\n",
            "Epoch 6: val_loss did not improve from 0.95108\n",
            "19/19 [==============================] - 6s 307ms/step - loss: 0.8710 - dice_coef: 0.1290 - iou: 0.0690 - recall_2: 0.2020 - precision_2: 0.6446 - val_loss: 0.9551 - val_dice_coef: 0.0449 - val_iou: 0.0230 - val_recall_2: 0.1603 - val_precision_2: 0.2633 - lr: 1.0000e-04\n",
            "Epoch 7/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.8661 - dice_coef: 0.1339 - iou: 0.0718 - recall_2: 0.1979 - precision_2: 0.6543\n",
            "Epoch 7: val_loss did not improve from 0.95108\n",
            "19/19 [==============================] - 6s 296ms/step - loss: 0.8661 - dice_coef: 0.1339 - iou: 0.0718 - recall_2: 0.1979 - precision_2: 0.6543 - val_loss: 0.9586 - val_dice_coef: 0.0414 - val_iou: 0.0212 - val_recall_2: 0.1120 - val_precision_2: 0.2682 - lr: 1.0000e-04\n",
            "Epoch 8/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.8581 - dice_coef: 0.1419 - iou: 0.0764 - recall_2: 0.1920 - precision_2: 0.6847\n",
            "Epoch 8: val_loss did not improve from 0.95108\n",
            "19/19 [==============================] - 6s 297ms/step - loss: 0.8581 - dice_coef: 0.1419 - iou: 0.0764 - recall_2: 0.1920 - precision_2: 0.6847 - val_loss: 0.9689 - val_dice_coef: 0.0311 - val_iou: 0.0159 - val_recall_2: 0.0512 - val_precision_2: 0.4144 - lr: 1.0000e-04\n",
            "Epoch 9/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.8541 - dice_coef: 0.1459 - iou: 0.0788 - recall_2: 0.1941 - precision_2: 0.6950\n",
            "Epoch 9: val_loss improved from 0.95108 to 0.94579, saving model to /content/drive/MyDrive/Colab Notebooks/Dataset/new_data/files/model1.h5\n",
            "19/19 [==============================] - 9s 489ms/step - loss: 0.8541 - dice_coef: 0.1459 - iou: 0.0788 - recall_2: 0.1941 - precision_2: 0.6950 - val_loss: 0.9458 - val_dice_coef: 0.0542 - val_iou: 0.0279 - val_recall_2: 0.1342 - val_precision_2: 0.3735 - lr: 1.0000e-04\n",
            "Epoch 10/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.8468 - dice_coef: 0.1532 - iou: 0.0830 - recall_2: 0.1812 - precision_2: 0.6844\n",
            "Epoch 10: val_loss improved from 0.94579 to 0.94120, saving model to /content/drive/MyDrive/Colab Notebooks/Dataset/new_data/files/model1.h5\n",
            "19/19 [==============================] - 7s 383ms/step - loss: 0.8468 - dice_coef: 0.1532 - iou: 0.0830 - recall_2: 0.1812 - precision_2: 0.6844 - val_loss: 0.9412 - val_dice_coef: 0.0588 - val_iou: 0.0303 - val_recall_2: 0.1637 - val_precision_2: 0.3478 - lr: 1.0000e-04\n",
            "Epoch 11/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.8373 - dice_coef: 0.1627 - iou: 0.0886 - recall_2: 0.1802 - precision_2: 0.7268\n",
            "Epoch 11: val_loss improved from 0.94120 to 0.94105, saving model to /content/drive/MyDrive/Colab Notebooks/Dataset/new_data/files/model1.h5\n",
            "19/19 [==============================] - 7s 385ms/step - loss: 0.8373 - dice_coef: 0.1627 - iou: 0.0886 - recall_2: 0.1802 - precision_2: 0.7268 - val_loss: 0.9410 - val_dice_coef: 0.0590 - val_iou: 0.0304 - val_recall_2: 0.1777 - val_precision_2: 0.3275 - lr: 1.0000e-04\n",
            "Epoch 12/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.8309 - dice_coef: 0.1691 - iou: 0.0925 - recall_2: 0.1727 - precision_2: 0.7233\n",
            "Epoch 12: val_loss improved from 0.94105 to 0.92581, saving model to /content/drive/MyDrive/Colab Notebooks/Dataset/new_data/files/model1.h5\n",
            "19/19 [==============================] - 7s 386ms/step - loss: 0.8309 - dice_coef: 0.1691 - iou: 0.0925 - recall_2: 0.1727 - precision_2: 0.7233 - val_loss: 0.9258 - val_dice_coef: 0.0742 - val_iou: 0.0386 - val_recall_2: 0.1737 - val_precision_2: 0.4791 - lr: 1.0000e-04\n",
            "Epoch 13/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.8238 - dice_coef: 0.1762 - iou: 0.0967 - recall_2: 0.1681 - precision_2: 0.7323\n",
            "Epoch 13: val_loss did not improve from 0.92581\n",
            "19/19 [==============================] - 6s 299ms/step - loss: 0.8238 - dice_coef: 0.1762 - iou: 0.0967 - recall_2: 0.1681 - precision_2: 0.7323 - val_loss: 0.9360 - val_dice_coef: 0.0640 - val_iou: 0.0331 - val_recall_2: 0.1821 - val_precision_2: 0.3334 - lr: 1.0000e-04\n",
            "Epoch 14/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.8235 - dice_coef: 0.1765 - iou: 0.0969 - recall_2: 0.1700 - precision_2: 0.7335\n",
            "Epoch 14: val_loss improved from 0.92581 to 0.91680, saving model to /content/drive/MyDrive/Colab Notebooks/Dataset/new_data/files/model1.h5\n",
            "19/19 [==============================] - 9s 485ms/step - loss: 0.8235 - dice_coef: 0.1765 - iou: 0.0969 - recall_2: 0.1700 - precision_2: 0.7335 - val_loss: 0.9168 - val_dice_coef: 0.0832 - val_iou: 0.0435 - val_recall_2: 0.1873 - val_precision_2: 0.4208 - lr: 1.0000e-04\n",
            "Epoch 15/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.8111 - dice_coef: 0.1889 - iou: 0.1045 - recall_2: 0.1568 - precision_2: 0.7636\n",
            "Epoch 15: val_loss improved from 0.91680 to 0.91206, saving model to /content/drive/MyDrive/Colab Notebooks/Dataset/new_data/files/model1.h5\n",
            "19/19 [==============================] - 7s 374ms/step - loss: 0.8111 - dice_coef: 0.1889 - iou: 0.1045 - recall_2: 0.1568 - precision_2: 0.7636 - val_loss: 0.9121 - val_dice_coef: 0.0879 - val_iou: 0.0462 - val_recall_2: 0.1713 - val_precision_2: 0.4555 - lr: 1.0000e-04\n",
            "Epoch 16/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.7985 - dice_coef: 0.2015 - iou: 0.1123 - recall_2: 0.1534 - precision_2: 0.7920\n",
            "Epoch 16: val_loss did not improve from 0.91206\n",
            "19/19 [==============================] - 6s 313ms/step - loss: 0.7985 - dice_coef: 0.2015 - iou: 0.1123 - recall_2: 0.1534 - precision_2: 0.7920 - val_loss: 0.9146 - val_dice_coef: 0.0854 - val_iou: 0.0448 - val_recall_2: 0.1606 - val_precision_2: 0.4149 - lr: 1.0000e-04\n",
            "Epoch 17/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.7964 - dice_coef: 0.2036 - iou: 0.1135 - recall_2: 0.1513 - precision_2: 0.7818\n",
            "Epoch 17: val_loss did not improve from 0.91206\n",
            "19/19 [==============================] - 6s 307ms/step - loss: 0.7964 - dice_coef: 0.2036 - iou: 0.1135 - recall_2: 0.1513 - precision_2: 0.7818 - val_loss: 0.9258 - val_dice_coef: 0.0742 - val_iou: 0.0386 - val_recall_2: 0.1647 - val_precision_2: 0.3825 - lr: 1.0000e-04\n",
            "Epoch 18/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.7886 - dice_coef: 0.2114 - iou: 0.1184 - recall_2: 0.1486 - precision_2: 0.7925\n",
            "Epoch 18: val_loss improved from 0.91206 to 0.90645, saving model to /content/drive/MyDrive/Colab Notebooks/Dataset/new_data/files/model1.h5\n",
            "19/19 [==============================] - 9s 495ms/step - loss: 0.7886 - dice_coef: 0.2114 - iou: 0.1184 - recall_2: 0.1486 - precision_2: 0.7925 - val_loss: 0.9064 - val_dice_coef: 0.0936 - val_iou: 0.0492 - val_recall_2: 0.1793 - val_precision_2: 0.3972 - lr: 1.0000e-04\n",
            "Epoch 19/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.7824 - dice_coef: 0.2176 - iou: 0.1224 - recall_2: 0.1482 - precision_2: 0.7935\n",
            "Epoch 19: val_loss improved from 0.90645 to 0.89883, saving model to /content/drive/MyDrive/Colab Notebooks/Dataset/new_data/files/model1.h5\n",
            "19/19 [==============================] - 7s 382ms/step - loss: 0.7824 - dice_coef: 0.2176 - iou: 0.1224 - recall_2: 0.1482 - precision_2: 0.7935 - val_loss: 0.8988 - val_dice_coef: 0.1012 - val_iou: 0.0534 - val_recall_2: 0.1235 - val_precision_2: 0.5286 - lr: 1.0000e-04\n",
            "Epoch 20/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.7706 - dice_coef: 0.2294 - iou: 0.1299 - recall_2: 0.1420 - precision_2: 0.8205\n",
            "Epoch 20: val_loss did not improve from 0.89883\n",
            "19/19 [==============================] - 6s 309ms/step - loss: 0.7706 - dice_coef: 0.2294 - iou: 0.1299 - recall_2: 0.1420 - precision_2: 0.8205 - val_loss: 0.9070 - val_dice_coef: 0.0930 - val_iou: 0.0489 - val_recall_2: 0.1545 - val_precision_2: 0.4391 - lr: 1.0000e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test1_model_weight = model1.get_weights()"
      ],
      "metadata": {
        "id": "WLwB-1anhnlc"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UvGRtU4XzSS9"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model2 (test)"
      ],
      "metadata": {
        "id": "l0BoWqctCnyE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, Input\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def conv_block(inputs, num_filters):\n",
        "    x = Conv2D(num_filters, 3, padding=\"same\")(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "\n",
        "    x = Conv2D(num_filters, 3, padding=\"same\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "def encoder_block(inputs, num_filters):\n",
        "    x = conv_block(inputs, num_filters)\n",
        "    p = MaxPool2D((2, 2))(x)\n",
        "    return x, p\n",
        "\n",
        "def decoder_block(inputs, skip_features, num_filters):\n",
        "    x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(inputs)\n",
        "    x = Concatenate()([x, skip_features])\n",
        "    x = conv_block(x, num_filters)\n",
        "    return x\n",
        "\n",
        "def build_unet_2(input_shape):\n",
        "    inputs = Input(input_shape)\n",
        "\n",
        "    s1, p1 = encoder_block(inputs, 64)\n",
        "    s2, p2 = encoder_block(p1, 128)\n",
        "    s3, p3 = encoder_block(p2, 256)\n",
        "    s4, p4 = encoder_block(p3, 512)\n",
        "\n",
        "    b1 = conv_block(p4, 1024)\n",
        "\n",
        "    d1 = decoder_block(b1, s4, 512)\n",
        "    d2 = decoder_block(d1, s3, 256)\n",
        "    d3 = decoder_block(d2, s2, 128)\n",
        "    d4 = decoder_block(d3, s1, 64)\n",
        "\n",
        "    outputs = Conv2D(1, 1, padding=\"same\", activation=\"sigmoid\")(d4)\n",
        "\n",
        "    model = Model(inputs, outputs, name=\"UNET\")\n",
        "    return model\n",
        "    "
      ],
      "metadata": {
        "id": "NGQTp3l7CprR"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
        "import numpy as np\n",
        "import cv2\n",
        "from glob import glob\n",
        "from sklearn.utils import shuffle\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau, EarlyStopping, TensorBoard\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import Recall, Precision\n",
        "\n",
        "\n",
        "H = 512\n",
        "W = 512\n",
        "\n",
        "def create_dir(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "def load_data(path):\n",
        "    x = sorted(glob(os.path.join(path, \"set3\",\"image\", \"*\")))\n",
        "    y = sorted(glob(os.path.join(path, \"set3\",\"mask\", \"*\")))\n",
        "    return x, y\n",
        "\n",
        "def shuffling(x, y):\n",
        "    x, y = shuffle(x, y, random_state=42)\n",
        "    return x, y\n",
        "\n",
        "def read_image(path):\n",
        "    path = path.decode()\n",
        "    x = cv2.imread(path, cv2.IMREAD_COLOR)\n",
        "    # x = cv2.resize(x, (W, H))\n",
        "    x = x/255.0\n",
        "    x = x.astype(np.float32)\n",
        "    return x\n",
        "\n",
        "def read_mask(path):\n",
        "    path = path.decode()\n",
        "    x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)  ## (512, 512)\n",
        "    # x = cv2.resize(x, (W, H))\n",
        "    x = x/255.0\n",
        "    x = x.astype(np.float32)\n",
        "    x = np.expand_dims(x, axis=-1)              ## (512, 512, 1)\n",
        "    return x\n",
        "\n",
        "def tf_parse(x, y):\n",
        "    def _parse(x, y):\n",
        "        x = read_image(x)\n",
        "        y = read_mask(y)\n",
        "        return x, y\n",
        "\n",
        "    x, y = tf.numpy_function(_parse, [x, y], [tf.float32, tf.float32])\n",
        "    x.set_shape([H, W, 3])\n",
        "    y.set_shape([H, W, 1])\n",
        "    return x, y\n",
        "\n",
        "def tf_dataset(X, Y, batch_size=2):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X, Y))\n",
        "    dataset = dataset.map(tf_parse)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(4)\n",
        "    return dataset\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    \"\"\" Seeding \"\"\"\n",
        "    np.random.seed(42)\n",
        "    tf.random.set_seed(42)\n",
        "\n",
        "    \"\"\" Directory to save files \"\"\"\n",
        "    create_dir(\"/content/drive/MyDrive/Colab Notebooks/Dataset/new_data/files\")\n",
        "\n",
        "    \"\"\" Hyperparameters \"\"\"\n",
        "    batch_size = 1\n",
        "    lr = 1e-4\n",
        "    num_epochs = 20\n",
        "    model_path = os.path.join(\"/content/drive/MyDrive/Colab Notebooks/Dataset/new_data/files\", \"model2.h5\")\n",
        "    csv_path = os.path.join(\"/content/drive/MyDrive/Colab Notebooks/Dataset/new_data/files\", \"data2.csv\")\n",
        "\n",
        "    \"\"\" Dataset \"\"\"\n",
        "    dataset_path = \"/content/drive/MyDrive/Colab Notebooks/Dataset/new_data\"\n",
        "    train_path = os.path.join(dataset_path, \"train\")\n",
        "    valid_path = os.path.join(dataset_path, \"valid\")\n",
        "\n",
        "    train_x, train_y = load_data(train_path)\n",
        "    train_x, train_y = shuffling(train_x, train_y)\n",
        "    valid_x, valid_y = load_data(valid_path)\n",
        "\n",
        "    print(f\"Train: {len(train_x)} - {len(train_y)}\")\n",
        "    print(f\"Valid: {len(valid_x)} - {len(valid_y)}\")\n",
        "\n",
        "    train_dataset = tf_dataset(train_x, train_y, batch_size=batch_size)\n",
        "    valid_dataset = tf_dataset(valid_x, valid_y, batch_size=batch_size)\n",
        "\n",
        "    train_steps = len(train_x)//batch_size\n",
        "    valid_setps = len(valid_x)//batch_size\n",
        "\n",
        "    if len(train_x) % batch_size != 0:\n",
        "        train_steps += 1\n",
        "    if len(valid_x) % batch_size != 0:\n",
        "        valid_setps += 1\n",
        "\n",
        "    \"\"\" Model \"\"\"\n",
        "    model2 = build_unet_2((H, W, 3))\n",
        "    model2.set_weights(main_model_weight)\n",
        "    model2.compile(loss=dice_loss, optimizer=Adam(lr), metrics=[dice_coef, iou, Recall(), Precision()])\n",
        "    # model.summary()\n",
        "\n",
        "    callbacks = [\n",
        "        ModelCheckpoint(model_path, verbose=1, save_best_only=True),\n",
        "        ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1, patience=5, min_lr=1e-6, verbose=1),\n",
        "        CSVLogger(csv_path),\n",
        "        TensorBoard(),\n",
        "        EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=False)\n",
        "    ]\n",
        "\n",
        "    model1.fit(\n",
        "        train_dataset,\n",
        "        epochs=num_epochs,\n",
        "        validation_data=valid_dataset,\n",
        "        steps_per_epoch=train_steps,\n",
        "        validation_steps=valid_setps,\n",
        "        callbacks=callbacks\n",
        "    )"
      ],
      "metadata": {
        "id": "vlNSxh5LC5ZU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28fd6857-e6ab-40ad-bba1-ebd4d2fb2aec"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 19 - 19\n",
            "Valid: 5 - 5\n",
            "Epoch 1/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.8731 - dice_coef: 0.1269 - iou: 0.0680 - recall_2: 0.2288 - precision_2: 0.4885\n",
            "Epoch 1: val_loss improved from inf to 0.90304, saving model to /content/drive/MyDrive/Colab Notebooks/Dataset/new_data/files/model2.h5\n",
            "19/19 [==============================] - 22s 1s/step - loss: 0.8731 - dice_coef: 0.1269 - iou: 0.0680 - recall_2: 0.2288 - precision_2: 0.4885 - val_loss: 0.9030 - val_dice_coef: 0.0970 - val_iou: 0.0510 - val_recall_2: 0.3102 - val_precision_2: 0.4442 - lr: 1.0000e-04\n",
            "Epoch 2/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.8404 - dice_coef: 0.1596 - iou: 0.0869 - recall_2: 0.2609 - precision_2: 0.5912\n",
            "Epoch 2: val_loss did not improve from 0.90304\n",
            "19/19 [==============================] - 6s 306ms/step - loss: 0.8404 - dice_coef: 0.1596 - iou: 0.0869 - recall_2: 0.2609 - precision_2: 0.5912 - val_loss: 0.9646 - val_dice_coef: 0.0354 - val_iou: 0.0181 - val_recall_2: 0.2771 - val_precision_2: 0.1043 - lr: 1.0000e-04\n",
            "Epoch 3/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.8175 - dice_coef: 0.1825 - iou: 0.1006 - recall_2: 0.2708 - precision_2: 0.6533\n",
            "Epoch 3: val_loss improved from 0.90304 to 0.89832, saving model to /content/drive/MyDrive/Colab Notebooks/Dataset/new_data/files/model2.h5\n",
            "19/19 [==============================] - 8s 406ms/step - loss: 0.8175 - dice_coef: 0.1825 - iou: 0.1006 - recall_2: 0.2708 - precision_2: 0.6533 - val_loss: 0.8983 - val_dice_coef: 0.1017 - val_iou: 0.0537 - val_recall_2: 0.1375 - val_precision_2: 0.5441 - lr: 1.0000e-04\n",
            "Epoch 4/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.7991 - dice_coef: 0.2009 - iou: 0.1118 - recall_2: 0.2622 - precision_2: 0.6898\n",
            "Epoch 4: val_loss did not improve from 0.89832\n",
            "19/19 [==============================] - 6s 298ms/step - loss: 0.7991 - dice_coef: 0.2009 - iou: 0.1118 - recall_2: 0.2622 - precision_2: 0.6898 - val_loss: 0.9415 - val_dice_coef: 0.0585 - val_iou: 0.0302 - val_recall_2: 0.0489 - val_precision_2: 0.5478 - lr: 1.0000e-04\n",
            "Epoch 5/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.7901 - dice_coef: 0.2099 - iou: 0.1174 - recall_2: 0.2466 - precision_2: 0.7339\n",
            "Epoch 5: val_loss did not improve from 0.89832\n",
            "19/19 [==============================] - 6s 301ms/step - loss: 0.7901 - dice_coef: 0.2099 - iou: 0.1174 - recall_2: 0.2466 - precision_2: 0.7339 - val_loss: 0.9602 - val_dice_coef: 0.0398 - val_iou: 0.0204 - val_recall_2: 0.0238 - val_precision_2: 0.5635 - lr: 1.0000e-04\n",
            "Epoch 6/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.7822 - dice_coef: 0.2178 - iou: 0.1225 - recall_2: 0.2544 - precision_2: 0.7205\n",
            "Epoch 6: val_loss did not improve from 0.89832\n",
            "19/19 [==============================] - 6s 308ms/step - loss: 0.7822 - dice_coef: 0.2178 - iou: 0.1225 - recall_2: 0.2544 - precision_2: 0.7205 - val_loss: 0.9610 - val_dice_coef: 0.0390 - val_iou: 0.0200 - val_recall_2: 0.0254 - val_precision_2: 0.5013 - lr: 1.0000e-04\n",
            "Epoch 7/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.7769 - dice_coef: 0.2231 - iou: 0.1258 - recall_2: 0.2381 - precision_2: 0.7427\n",
            "Epoch 7: val_loss did not improve from 0.89832\n",
            "19/19 [==============================] - 6s 310ms/step - loss: 0.7769 - dice_coef: 0.2231 - iou: 0.1258 - recall_2: 0.2381 - precision_2: 0.7427 - val_loss: 0.9870 - val_dice_coef: 0.0130 - val_iou: 0.0065 - val_recall_2: 0.0000e+00 - val_precision_2: 0.0000e+00 - lr: 1.0000e-04\n",
            "Epoch 8/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.7676 - dice_coef: 0.2324 - iou: 0.1317 - recall_2: 0.2423 - precision_2: 0.7630\n",
            "Epoch 8: val_loss did not improve from 0.89832\n",
            "\n",
            "Epoch 8: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
            "19/19 [==============================] - 6s 308ms/step - loss: 0.7676 - dice_coef: 0.2324 - iou: 0.1317 - recall_2: 0.2423 - precision_2: 0.7630 - val_loss: 0.9660 - val_dice_coef: 0.0340 - val_iou: 0.0177 - val_recall_2: 0.0197 - val_precision_2: 0.6880 - lr: 1.0000e-04\n",
            "Epoch 9/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.7406 - dice_coef: 0.2594 - iou: 0.1493 - recall_2: 0.2568 - precision_2: 0.8096\n",
            "Epoch 9: val_loss did not improve from 0.89832\n",
            "19/19 [==============================] - 6s 298ms/step - loss: 0.7406 - dice_coef: 0.2594 - iou: 0.1493 - recall_2: 0.2568 - precision_2: 0.8096 - val_loss: 0.9719 - val_dice_coef: 0.0281 - val_iou: 0.0145 - val_recall_2: 0.0106 - val_precision_2: 0.8057 - lr: 1.0000e-05\n",
            "Epoch 10/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.7206 - dice_coef: 0.2794 - iou: 0.1627 - recall_2: 0.2472 - precision_2: 0.8571\n",
            "Epoch 10: val_loss did not improve from 0.89832\n",
            "19/19 [==============================] - 6s 297ms/step - loss: 0.7206 - dice_coef: 0.2794 - iou: 0.1627 - recall_2: 0.2472 - precision_2: 0.8571 - val_loss: 0.9693 - val_dice_coef: 0.0307 - val_iou: 0.0159 - val_recall_2: 0.0149 - val_precision_2: 0.7410 - lr: 1.0000e-05\n",
            "Epoch 11/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.7060 - dice_coef: 0.2940 - iou: 0.1725 - recall_2: 0.2389 - precision_2: 0.8846\n",
            "Epoch 11: val_loss did not improve from 0.89832\n",
            "19/19 [==============================] - 6s 308ms/step - loss: 0.7060 - dice_coef: 0.2940 - iou: 0.1725 - recall_2: 0.2389 - precision_2: 0.8846 - val_loss: 0.9706 - val_dice_coef: 0.0294 - val_iou: 0.0152 - val_recall_2: 0.0143 - val_precision_2: 0.6988 - lr: 1.0000e-05\n",
            "Epoch 12/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.6955 - dice_coef: 0.3045 - iou: 0.1798 - recall_2: 0.2274 - precision_2: 0.9062\n",
            "Epoch 12: val_loss did not improve from 0.89832\n",
            "19/19 [==============================] - 6s 295ms/step - loss: 0.6955 - dice_coef: 0.3045 - iou: 0.1798 - recall_2: 0.2274 - precision_2: 0.9062 - val_loss: 0.9722 - val_dice_coef: 0.0278 - val_iou: 0.0143 - val_recall_2: 0.0130 - val_precision_2: 0.6700 - lr: 1.0000e-05\n",
            "Epoch 13/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.6869 - dice_coef: 0.3131 - iou: 0.1858 - recall_2: 0.2222 - precision_2: 0.9213\n",
            "Epoch 13: val_loss did not improve from 0.89832\n",
            "\n",
            "Epoch 13: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
            "19/19 [==============================] - 6s 305ms/step - loss: 0.6869 - dice_coef: 0.3131 - iou: 0.1858 - recall_2: 0.2222 - precision_2: 0.9213 - val_loss: 0.9707 - val_dice_coef: 0.0293 - val_iou: 0.0150 - val_recall_2: 0.0140 - val_precision_2: 0.6576 - lr: 1.0000e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test2_model_weight = model2.get_weights()"
      ],
      "metadata": {
        "id": "94uRPwQQDhW5"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "addition = [x + y for (x, y) in zip(test1_model_weight, test2_model_weight)] "
      ],
      "metadata": {
        "id": "KQ5DRV2_DtzK"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "average_weight = [x / 2 for x in addition]"
      ],
      "metadata": {
        "id": "P_Is2CJDIDsW"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test1_model_weight[1]"
      ],
      "metadata": {
        "id": "dUyBiDfWLAoL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20b757dd-c560-4373-f86c-6a19231b3de8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-2.5553283e-05,  1.0903518e-05, -4.5498609e-04, -1.3972581e-03,\n",
              "        1.2187082e-03, -1.0818432e-03, -3.7178395e-03, -5.3266118e-05,\n",
              "        1.8180141e-04, -1.6808416e-03, -3.1459488e-03,  6.4421189e-04,\n",
              "       -2.8726330e-03,  6.0499879e-04, -1.5771989e-03, -9.2214199e-05,\n",
              "        5.6727428e-04, -2.1721255e-04,  3.6450175e-03,  7.6650955e-05,\n",
              "       -2.0863409e-03, -1.9540582e-03, -3.1556194e-03, -2.1723111e-04,\n",
              "       -1.8162451e-05,  2.5621994e-04, -2.3599612e-03,  1.2184231e-04,\n",
              "        2.0281810e-03, -1.3034323e-03,  6.9877418e-04,  6.6290126e-04,\n",
              "       -4.8778056e-06, -1.8893725e-05,  1.0964203e-03,  1.3400710e-04,\n",
              "        1.5873984e-03,  8.7676418e-04,  3.9802809e-04,  1.2219038e-03,\n",
              "        3.4696259e-05, -2.5322225e-03,  1.0542824e-03,  1.3516650e-03,\n",
              "       -7.9937257e-05,  2.5385962e-04,  1.1535315e-05,  1.0736098e-04,\n",
              "        1.2111211e-03, -3.2475991e-03, -4.7778900e-04,  9.3452487e-05,\n",
              "       -1.7203639e-04, -1.4624388e-03,  1.1295434e-03, -9.5851300e-04,\n",
              "       -2.8022022e-03,  2.8478154e-03, -8.0163416e-05,  1.7377646e-03,\n",
              "       -6.3257758e-06,  1.0453377e-04,  5.9926498e-04,  4.5490140e-04],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test2_model_weight[1]"
      ],
      "metadata": {
        "id": "rqbtjnBGLBOb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "582cc122-fe42-4ca4-c512-2c75c1e02671"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1.77368158e-04, -5.95017846e-05,  1.56905531e-04,  6.63477927e-04,\n",
              "       -8.26787378e-04, -3.80951707e-04,  6.40103943e-04,  1.44280406e-04,\n",
              "        1.96476627e-04, -1.70962105e-03, -8.85390502e-04,  8.51202523e-04,\n",
              "        4.22853482e-04,  1.61454358e-04,  1.96811612e-04,  1.02018654e-04,\n",
              "        9.85602732e-04, -4.53178305e-04,  1.12265942e-03,  8.17199907e-05,\n",
              "        1.20600067e-04, -9.76447773e-04, -1.17283536e-03, -7.12048277e-05,\n",
              "        5.30514553e-05, -4.83056783e-06, -1.39297044e-03, -1.70432177e-04,\n",
              "       -9.30314476e-04, -1.86455625e-04,  6.83442995e-05, -1.15152914e-03,\n",
              "       -4.21561330e-04, -9.12194955e-05,  2.70050252e-04, -1.54758207e-04,\n",
              "       -1.23072741e-05, -2.88991287e-04,  1.21419587e-04,  1.21155242e-03,\n",
              "       -1.10057645e-05, -8.84760462e-04,  4.55492904e-04,  1.14833652e-04,\n",
              "        1.09752938e-04, -1.45919490e-04,  2.62234651e-04,  7.33638735e-05,\n",
              "       -4.03415062e-04,  5.85109694e-04, -1.38250834e-04, -1.16294203e-03,\n",
              "        6.49475260e-05, -3.43843945e-04,  1.43894833e-03, -1.36426912e-04,\n",
              "       -4.02010570e-04,  9.86786559e-04, -3.02507309e-04, -4.35269583e-04,\n",
              "       -8.63139576e-05,  3.98356424e-05,  7.28719053e-04,  1.42109522e-04],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "average_weight[1]"
      ],
      "metadata": {
        "id": "tSJHrHZ4KZ4N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "166eea55-866f-48af-b1bd-e8f0dbd4e758"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1.01460719e-04, -2.42991337e-05, -1.49040279e-04, -3.66890104e-04,\n",
              "        1.95960427e-04, -7.31397420e-04, -1.53886783e-03,  4.55071422e-05,\n",
              "        1.89139013e-04, -1.69523130e-03, -2.01566960e-03,  7.47707207e-04,\n",
              "       -1.22488977e-03,  3.83226579e-04, -6.90193614e-04,  4.90222737e-06,\n",
              "        7.76438508e-04, -3.35195422e-04,  2.38383841e-03,  7.91854691e-05,\n",
              "       -9.82870464e-04, -1.46525295e-03, -2.16422742e-03, -1.44217964e-04,\n",
              "        1.74445013e-05,  1.25694685e-04, -1.87646574e-03, -2.42949318e-05,\n",
              "        5.48933283e-04, -7.44943973e-04,  3.83559236e-04, -2.44313938e-04,\n",
              "       -2.13219566e-04, -5.50566110e-05,  6.83235296e-04, -1.03755519e-05,\n",
              "        7.87545519e-04,  2.93886464e-04,  2.59723834e-04,  1.21672812e-03,\n",
              "        1.18452472e-05, -1.70849147e-03,  7.54887646e-04,  7.33249355e-04,\n",
              "        1.49078405e-05,  5.39700632e-05,  1.36884977e-04,  9.03624314e-05,\n",
              "        4.03853017e-04, -1.33124471e-03, -3.08019924e-04, -5.34744759e-04,\n",
              "       -5.35444342e-05, -9.03141394e-04,  1.28424587e-03, -5.47469943e-04,\n",
              "       -1.60210638e-03,  1.91730098e-03, -1.91335363e-04,  6.51247508e-04,\n",
              "       -4.63198667e-05,  7.21847027e-05,  6.63992018e-04,  2.98505474e-04],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Tl4L1qvuK7jv"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Repeat with average weight in main model"
      ],
      "metadata": {
        "id": "6ndBtEd6LM5m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, Input\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def conv_block(inputs, num_filters):\n",
        "    x = Conv2D(num_filters, 3, padding=\"same\")(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "\n",
        "    x = Conv2D(num_filters, 3, padding=\"same\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "def encoder_block(inputs, num_filters):\n",
        "    x = conv_block(inputs, num_filters)\n",
        "    p = MaxPool2D((2, 2))(x)\n",
        "    return x, p\n",
        "\n",
        "def decoder_block(inputs, skip_features, num_filters):\n",
        "    x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(inputs)\n",
        "    x = Concatenate()([x, skip_features])\n",
        "    x = conv_block(x, num_filters)\n",
        "    return x\n",
        "\n",
        "def build_unet_main(input_shape):\n",
        "    inputs = Input(input_shape)\n",
        "\n",
        "    s1, p1 = encoder_block(inputs, 64)\n",
        "    s2, p2 = encoder_block(p1, 128)\n",
        "    s3, p3 = encoder_block(p2, 256)\n",
        "    s4, p4 = encoder_block(p3, 512)\n",
        "\n",
        "    b1 = conv_block(p4, 1024)\n",
        "\n",
        "    d1 = decoder_block(b1, s4, 512)\n",
        "    d2 = decoder_block(d1, s3, 256)\n",
        "    d3 = decoder_block(d2, s2, 128)\n",
        "    d4 = decoder_block(d3, s1, 64)\n",
        "\n",
        "    outputs = Conv2D(1, 1, padding=\"same\", activation=\"sigmoid\")(d4)\n",
        "\n",
        "    model = Model(inputs, outputs, name=\"UNET\")\n",
        "    return model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  input_shape = (512, 512, 3)\n",
        "  model_main = build_unet_main(input_shape)\n",
        "  "
      ],
      "metadata": {
        "id": "LiDVYOHjLjD7"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d8iQomAeLw-c"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAPTSY4YMChX"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "4OcOC7LTMChe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "509abd7a-9d3c-4cf0-9ae4-b19ea13ef7c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 19 - 19\n",
            "Valid: 5 - 5\n",
            "Epoch 1/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.8625 - dice_coef: 0.1375 - iou: 0.0739 - recall_4: 0.0804 - precision_4: 0.6404\n",
            "Epoch 1: val_loss improved from inf to 0.96437, saving model to /content/drive/MyDrive/Colab Notebooks/Dataset/new_data/files/model3.h5\n",
            "19/19 [==============================] - 19s 887ms/step - loss: 0.8625 - dice_coef: 0.1375 - iou: 0.0739 - recall_4: 0.0804 - precision_4: 0.6404 - val_loss: 0.9644 - val_dice_coef: 0.0356 - val_iou: 0.0182 - val_recall_4: 0.0118 - val_precision_4: 0.9034 - lr: 1.0000e-04\n",
            "Epoch 2/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.8434 - dice_coef: 0.1566 - iou: 0.0851 - recall_4: 0.0804 - precision_4: 0.6963\n",
            "Epoch 2: val_loss did not improve from 0.96437\n",
            "19/19 [==============================] - 6s 308ms/step - loss: 0.8434 - dice_coef: 0.1566 - iou: 0.0851 - recall_4: 0.0804 - precision_4: 0.6963 - val_loss: 0.9694 - val_dice_coef: 0.0306 - val_iou: 0.0155 - val_recall_4: 0.0096 - val_precision_4: 0.9749 - lr: 1.0000e-04\n",
            "Epoch 3/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.8325 - dice_coef: 0.1675 - iou: 0.0916 - recall_4: 0.0763 - precision_4: 0.7303\n",
            "Epoch 3: val_loss improved from 0.96437 to 0.95508, saving model to /content/drive/MyDrive/Colab Notebooks/Dataset/new_data/files/model3.h5\n",
            "19/19 [==============================] - 11s 566ms/step - loss: 0.8325 - dice_coef: 0.1675 - iou: 0.0916 - recall_4: 0.0763 - precision_4: 0.7303 - val_loss: 0.9551 - val_dice_coef: 0.0449 - val_iou: 0.0230 - val_recall_4: 0.0203 - val_precision_4: 0.9025 - lr: 1.0000e-04\n",
            "Epoch 4/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.8204 - dice_coef: 0.1796 - iou: 0.0988 - recall_4: 0.0742 - precision_4: 0.7531\n",
            "Epoch 4: val_loss improved from 0.95508 to 0.94502, saving model to /content/drive/MyDrive/Colab Notebooks/Dataset/new_data/files/model3.h5\n",
            "19/19 [==============================] - 11s 570ms/step - loss: 0.8204 - dice_coef: 0.1796 - iou: 0.0988 - recall_4: 0.0742 - precision_4: 0.7531 - val_loss: 0.9450 - val_dice_coef: 0.0550 - val_iou: 0.0284 - val_recall_4: 0.0156 - val_precision_4: 0.9824 - lr: 1.0000e-04\n",
            "Epoch 5/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.8196 - dice_coef: 0.1804 - iou: 0.0993 - recall_4: 0.0732 - precision_4: 0.7410\n",
            "Epoch 5: val_loss improved from 0.94502 to 0.94214, saving model to /content/drive/MyDrive/Colab Notebooks/Dataset/new_data/files/model3.h5\n",
            "19/19 [==============================] - 12s 636ms/step - loss: 0.8196 - dice_coef: 0.1804 - iou: 0.0993 - recall_4: 0.0732 - precision_4: 0.7410 - val_loss: 0.9421 - val_dice_coef: 0.0579 - val_iou: 0.0300 - val_recall_4: 0.0107 - val_precision_4: 0.9829 - lr: 1.0000e-04\n",
            "Epoch 6/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.8097 - dice_coef: 0.1903 - iou: 0.1054 - recall_4: 0.0710 - precision_4: 0.7586\n",
            "Epoch 6: val_loss improved from 0.94214 to 0.92375, saving model to /content/drive/MyDrive/Colab Notebooks/Dataset/new_data/files/model3.h5\n",
            "19/19 [==============================] - 11s 578ms/step - loss: 0.8097 - dice_coef: 0.1903 - iou: 0.1054 - recall_4: 0.0710 - precision_4: 0.7586 - val_loss: 0.9238 - val_dice_coef: 0.0762 - val_iou: 0.0400 - val_recall_4: 0.0201 - val_precision_4: 0.9110 - lr: 1.0000e-04\n",
            "Epoch 7/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.8005 - dice_coef: 0.1995 - iou: 0.1110 - recall_4: 0.0671 - precision_4: 0.7833\n",
            "Epoch 7: val_loss improved from 0.92375 to 0.88994, saving model to /content/drive/MyDrive/Colab Notebooks/Dataset/new_data/files/model3.h5\n",
            "19/19 [==============================] - 10s 546ms/step - loss: 0.8005 - dice_coef: 0.1995 - iou: 0.1110 - recall_4: 0.0671 - precision_4: 0.7833 - val_loss: 0.8899 - val_dice_coef: 0.1101 - val_iou: 0.0585 - val_recall_4: 0.0245 - val_precision_4: 0.9193 - lr: 1.0000e-04\n",
            "Epoch 8/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.7931 - dice_coef: 0.2069 - iou: 0.1157 - recall_4: 0.0641 - precision_4: 0.7774\n",
            "Epoch 8: val_loss improved from 0.88994 to 0.88535, saving model to /content/drive/MyDrive/Colab Notebooks/Dataset/new_data/files/model3.h5\n",
            "19/19 [==============================] - 7s 384ms/step - loss: 0.7931 - dice_coef: 0.2069 - iou: 0.1157 - recall_4: 0.0641 - precision_4: 0.7774 - val_loss: 0.8854 - val_dice_coef: 0.1146 - val_iou: 0.0611 - val_recall_4: 0.0336 - val_precision_4: 0.8740 - lr: 1.0000e-04\n",
            "Epoch 9/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.7858 - dice_coef: 0.2142 - iou: 0.1202 - recall_4: 0.0629 - precision_4: 0.7885\n",
            "Epoch 9: val_loss did not improve from 0.88535\n",
            "19/19 [==============================] - 6s 303ms/step - loss: 0.7858 - dice_coef: 0.2142 - iou: 0.1202 - recall_4: 0.0629 - precision_4: 0.7885 - val_loss: 0.8972 - val_dice_coef: 0.1028 - val_iou: 0.0547 - val_recall_4: 0.0179 - val_precision_4: 0.9321 - lr: 1.0000e-04\n",
            "Epoch 10/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.7771 - dice_coef: 0.2229 - iou: 0.1257 - recall_4: 0.0604 - precision_4: 0.8133\n",
            "Epoch 10: val_loss improved from 0.88535 to 0.87733, saving model to /content/drive/MyDrive/Colab Notebooks/Dataset/new_data/files/model3.h5\n",
            "19/19 [==============================] - 10s 561ms/step - loss: 0.7771 - dice_coef: 0.2229 - iou: 0.1257 - recall_4: 0.0604 - precision_4: 0.8133 - val_loss: 0.8773 - val_dice_coef: 0.1227 - val_iou: 0.0658 - val_recall_4: 0.0315 - val_precision_4: 0.8569 - lr: 1.0000e-04\n",
            "Epoch 11/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.7665 - dice_coef: 0.2335 - iou: 0.1325 - recall_4: 0.0577 - precision_4: 0.8288\n",
            "Epoch 11: val_loss did not improve from 0.87733\n",
            "19/19 [==============================] - 6s 306ms/step - loss: 0.7665 - dice_coef: 0.2335 - iou: 0.1325 - recall_4: 0.0577 - precision_4: 0.8288 - val_loss: 0.8844 - val_dice_coef: 0.1156 - val_iou: 0.0617 - val_recall_4: 0.0230 - val_precision_4: 0.9198 - lr: 1.0000e-04\n",
            "Epoch 12/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.7560 - dice_coef: 0.2440 - iou: 0.1394 - recall_4: 0.0580 - precision_4: 0.8419\n",
            "Epoch 12: val_loss did not improve from 0.87733\n",
            "19/19 [==============================] - 6s 306ms/step - loss: 0.7560 - dice_coef: 0.2440 - iou: 0.1394 - recall_4: 0.0580 - precision_4: 0.8419 - val_loss: 0.9057 - val_dice_coef: 0.0943 - val_iou: 0.0498 - val_recall_4: 0.0199 - val_precision_4: 0.8967 - lr: 1.0000e-04\n",
            "Epoch 13/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.7598 - dice_coef: 0.2402 - iou: 0.1368 - recall_4: 0.0562 - precision_4: 0.8263\n",
            "Epoch 13: val_loss improved from 0.87733 to 0.85281, saving model to /content/drive/MyDrive/Colab Notebooks/Dataset/new_data/files/model3.h5\n",
            "19/19 [==============================] - 10s 544ms/step - loss: 0.7598 - dice_coef: 0.2402 - iou: 0.1368 - recall_4: 0.0562 - precision_4: 0.8263 - val_loss: 0.8528 - val_dice_coef: 0.1472 - val_iou: 0.0797 - val_recall_4: 0.0351 - val_precision_4: 0.8767 - lr: 1.0000e-04\n",
            "Epoch 14/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.7506 - dice_coef: 0.2494 - iou: 0.1431 - recall_4: 0.0548 - precision_4: 0.8298\n",
            "Epoch 14: val_loss did not improve from 0.85281\n",
            "19/19 [==============================] - 6s 300ms/step - loss: 0.7506 - dice_coef: 0.2494 - iou: 0.1431 - recall_4: 0.0548 - precision_4: 0.8298 - val_loss: 0.8698 - val_dice_coef: 0.1302 - val_iou: 0.0700 - val_recall_4: 0.0318 - val_precision_4: 0.8370 - lr: 1.0000e-04\n",
            "Epoch 15/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.7515 - dice_coef: 0.2485 - iou: 0.1426 - recall_4: 0.0543 - precision_4: 0.8342\n",
            "Epoch 15: val_loss did not improve from 0.85281\n",
            "19/19 [==============================] - 6s 314ms/step - loss: 0.7515 - dice_coef: 0.2485 - iou: 0.1426 - recall_4: 0.0543 - precision_4: 0.8342 - val_loss: 0.9078 - val_dice_coef: 0.0922 - val_iou: 0.0485 - val_recall_4: 0.0148 - val_precision_4: 0.8839 - lr: 1.0000e-04\n",
            "Epoch 16/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.7267 - dice_coef: 0.2733 - iou: 0.1589 - recall_4: 0.0525 - precision_4: 0.8712\n",
            "Epoch 16: val_loss did not improve from 0.85281\n",
            "19/19 [==============================] - 6s 299ms/step - loss: 0.7267 - dice_coef: 0.2733 - iou: 0.1589 - recall_4: 0.0525 - precision_4: 0.8712 - val_loss: 0.8749 - val_dice_coef: 0.1251 - val_iou: 0.0670 - val_recall_4: 0.0254 - val_precision_4: 0.8539 - lr: 1.0000e-04\n",
            "Epoch 17/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.7135 - dice_coef: 0.2865 - iou: 0.1678 - recall_4: 0.0494 - precision_4: 0.8902\n",
            "Epoch 17: val_loss did not improve from 0.85281\n",
            "19/19 [==============================] - 6s 301ms/step - loss: 0.7135 - dice_coef: 0.2865 - iou: 0.1678 - recall_4: 0.0494 - precision_4: 0.8902 - val_loss: 0.8895 - val_dice_coef: 0.1105 - val_iou: 0.0587 - val_recall_4: 0.0234 - val_precision_4: 0.8594 - lr: 1.0000e-04\n",
            "Epoch 18/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.7047 - dice_coef: 0.2953 - iou: 0.1740 - recall_4: 0.0504 - precision_4: 0.9041\n",
            "Epoch 18: val_loss did not improve from 0.85281\n",
            "\n",
            "Epoch 18: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
            "19/19 [==============================] - 6s 308ms/step - loss: 0.7047 - dice_coef: 0.2953 - iou: 0.1740 - recall_4: 0.0504 - precision_4: 0.9041 - val_loss: 0.9032 - val_dice_coef: 0.0968 - val_iou: 0.0512 - val_recall_4: 0.0174 - val_precision_4: 0.8817 - lr: 1.0000e-04\n",
            "Epoch 19/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.7023 - dice_coef: 0.2977 - iou: 0.1755 - recall_4: 0.0499 - precision_4: 0.9140\n",
            "Epoch 19: val_loss did not improve from 0.85281\n",
            "19/19 [==============================] - 6s 298ms/step - loss: 0.7023 - dice_coef: 0.2977 - iou: 0.1755 - recall_4: 0.0499 - precision_4: 0.9140 - val_loss: 0.8666 - val_dice_coef: 0.1334 - val_iou: 0.0719 - val_recall_4: 0.0247 - val_precision_4: 0.8829 - lr: 1.0000e-05\n",
            "Epoch 20/20\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.6692 - dice_coef: 0.3308 - iou: 0.1989 - recall_4: 0.0512 - precision_4: 0.9486\n",
            "Epoch 20: val_loss did not improve from 0.85281\n",
            "19/19 [==============================] - 6s 313ms/step - loss: 0.6692 - dice_coef: 0.3308 - iou: 0.1989 - recall_4: 0.0512 - precision_4: 0.9486 - val_loss: 0.8771 - val_dice_coef: 0.1229 - val_iou: 0.0659 - val_recall_4: 0.0201 - val_precision_4: 0.8941 - lr: 1.0000e-05\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
        "import numpy as np\n",
        "import cv2\n",
        "from glob import glob\n",
        "from sklearn.utils import shuffle\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau, EarlyStopping, TensorBoard\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import Recall, Precision\n",
        "\n",
        "\n",
        "H = 512\n",
        "W = 512\n",
        "\n",
        "def create_dir(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "def load_data(path):\n",
        "    x = sorted(glob(os.path.join(path, \"set1\",\"image\", \"*\")))\n",
        "    y = sorted(glob(os.path.join(path, \"set1\",\"mask\", \"*\")))\n",
        "    return x, y\n",
        "\n",
        "def shuffling(x, y):\n",
        "    x, y = shuffle(x, y, random_state=42)\n",
        "    return x, y\n",
        "\n",
        "def read_image(path):\n",
        "    path = path.decode()\n",
        "    x = cv2.imread(path, cv2.IMREAD_COLOR)\n",
        "    # x = cv2.resize(x, (W, H))\n",
        "    x = x/255.0\n",
        "    x = x.astype(np.float32)\n",
        "    return x\n",
        "\n",
        "def read_mask(path):\n",
        "    path = path.decode()\n",
        "    x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)  ## (512, 512)\n",
        "    # x = cv2.resize(x, (W, H))\n",
        "    x = x/255.0\n",
        "    x = x.astype(np.float32)\n",
        "    x = np.expand_dims(x, axis=-1)              ## (512, 512, 1)\n",
        "    return x\n",
        "\n",
        "def tf_parse(x, y):\n",
        "    def _parse(x, y):\n",
        "        x = read_image(x)\n",
        "        y = read_mask(y)\n",
        "        return x, y\n",
        "\n",
        "    x, y = tf.numpy_function(_parse, [x, y], [tf.float32, tf.float32])\n",
        "    x.set_shape([H, W, 3])\n",
        "    y.set_shape([H, W, 1])\n",
        "    return x, y\n",
        "\n",
        "def tf_dataset(X, Y, batch_size=2):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X, Y))\n",
        "    dataset = dataset.map(tf_parse)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(4)\n",
        "    return dataset\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    \"\"\" Seeding \"\"\"\n",
        "    np.random.seed(42)\n",
        "    tf.random.set_seed(42)\n",
        "\n",
        "    \"\"\" Directory to save files \"\"\"\n",
        "    create_dir(\"/content/drive/MyDrive/Colab Notebooks/Dataset/new_data/files\")\n",
        "\n",
        "    \"\"\" Hyperparameters \"\"\"\n",
        "    batch_size = 1\n",
        "    lr = 1e-4\n",
        "    num_epochs = 20\n",
        "    model_path = os.path.join(\"/content/drive/MyDrive/Colab Notebooks/Dataset/new_data/files\", \"model3.h5\")\n",
        "    csv_path = os.path.join(\"/content/drive/MyDrive/Colab Notebooks/Dataset/new_data/files\", \"data3.csv\")\n",
        "\n",
        "    \"\"\" Dataset \"\"\"\n",
        "    dataset_path = \"/content/drive/MyDrive/Colab Notebooks/Dataset/new_data\"\n",
        "    train_path = os.path.join(dataset_path, \"train\")\n",
        "    valid_path = os.path.join(dataset_path, \"valid\")\n",
        "\n",
        "    train_x, train_y = load_data(train_path)\n",
        "    train_x, train_y = shuffling(train_x, train_y)\n",
        "    valid_x, valid_y = load_data(valid_path)\n",
        "\n",
        "    print(f\"Train: {len(train_x)} - {len(train_y)}\")\n",
        "    print(f\"Valid: {len(valid_x)} - {len(valid_y)}\")\n",
        "\n",
        "    train_dataset = tf_dataset(train_x, train_y, batch_size=batch_size)\n",
        "    valid_dataset = tf_dataset(valid_x, valid_y, batch_size=batch_size)\n",
        "\n",
        "    train_steps = len(train_x)//batch_size\n",
        "    valid_setps = len(valid_x)//batch_size\n",
        "\n",
        "    if len(train_x) % batch_size != 0:\n",
        "        train_steps += 1\n",
        "    if len(valid_x) % batch_size != 0:\n",
        "        valid_setps += 1\n",
        "\n",
        "    \"\"\" Model \"\"\"\n",
        "    model_main = build_unet_main((H, W, 3))\n",
        "    model_main.set_weights(average_weight)\n",
        "    model_main.compile(loss=dice_loss, optimizer=Adam(lr), metrics=[dice_coef, iou, Recall(), Precision()])\n",
        "    # model.summary()\n",
        "\n",
        "    callbacks = [\n",
        "        ModelCheckpoint(model_path, verbose=1, save_best_only=True),\n",
        "        ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1, patience=5, min_lr=1e-6, verbose=1),\n",
        "        CSVLogger(csv_path),\n",
        "        TensorBoard(),\n",
        "        EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=False)\n",
        "    ]\n",
        "\n",
        "    model_main.fit(\n",
        "        train_dataset,\n",
        "        epochs=num_epochs,\n",
        "        validation_data=valid_dataset,\n",
        "        steps_per_epoch=train_steps,\n",
        "        validation_steps=valid_setps,\n",
        "        callbacks=callbacks\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "p2aFicCFMChf"
      },
      "outputs": [],
      "source": [
        "#main_model_weight=model_main.get_weights()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcJW5uEwMChf"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "86qDHJQeMChf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4e19769-bce3-42c5-9bc4-380786a3545e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 1/10 [00:01<00:13,  1.51s/it]/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            " 20%|██        | 2/10 [00:02<00:08,  1.05s/it]/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            " 30%|███       | 3/10 [00:03<00:06,  1.08it/s]/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            " 40%|████      | 4/10 [00:03<00:05,  1.16it/s]/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            " 50%|█████     | 5/10 [00:04<00:04,  1.21it/s]/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            " 60%|██████    | 6/10 [00:05<00:03,  1.15it/s]/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            " 70%|███████   | 7/10 [00:06<00:02,  1.17it/s]/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            " 80%|████████  | 8/10 [00:07<00:01,  1.20it/s]/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            " 90%|█████████ | 9/10 [00:07<00:00,  1.23it/s]/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "100%|██████████| 10/10 [00:08<00:00,  1.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 0.98651\n",
            "F1: 0.49672\n",
            "Jaccard: 0.49331\n",
            "Recall: 0.53611\n",
            "Precision: 0.50006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import CustomObjectScope\n",
        "from sklearn.metrics import accuracy_score, f1_score, jaccard_score, precision_score, recall_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "H = 512\n",
        "W = 512\n",
        "\n",
        "def create_dir(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "def read_image(path):\n",
        "    x = cv2.imread(path, cv2.IMREAD_COLOR)\n",
        "    # x = cv2.resize(x, (W, H))\n",
        "    ori_x = x\n",
        "    x = x/255.0\n",
        "    x = x.astype(np.float32)\n",
        "    return ori_x, x\n",
        "\n",
        "def read_mask(path):\n",
        "    y = cv2.imread(path, cv2.IMREAD_GRAYSCALE)  ## (512, 512)\n",
        "    # x = cv2.resize(x, (W, H))\n",
        "    ori_y = y\n",
        "    y = y/255.0\n",
        "    y = y.astype(np.int32)\n",
        "    return ori_y, y\n",
        "\n",
        "\n",
        "\n",
        "def save_results(ori_x, ori_y, y_pred, save_image_path):\n",
        "    line = np.ones((H, 10, 3)) * 255\n",
        "\n",
        "    ori_y = np.expand_dims(ori_y, axis=-1)\n",
        "    ori_y = np.concatenate([ori_y, ori_y, ori_y], axis=-1)\n",
        "\n",
        "    y_pred = np.expand_dims(y_pred, axis=-1)\n",
        "    y_pred = np.concatenate([y_pred, y_pred, y_pred], axis=-1) * 255\n",
        "\n",
        "    cat_images = np.concatenate([ori_x, line, ori_y, line, y_pred], axis=1)\n",
        "    cv2.imwrite(save_image_path, cat_images)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    \"\"\" Save the results in this folder \"\"\"\n",
        "    create_dir(\"/content/drive/MyDrive/Colab Notebooks/Dataset/new_data/results2\")\n",
        "\n",
        "    \"\"\" Load the model \"\"\"\n",
        "    with CustomObjectScope({'iou': iou, 'dice_coef': dice_coef, 'dice_loss': dice_loss}):\n",
        "        model = tf.keras.models.load_model(\"/content/drive/MyDrive/Colab Notebooks/Dataset/new_data/files/model3.h5\")\n",
        "\n",
        "    \"\"\" Load the dataset \"\"\"\n",
        "    test_x = sorted(glob(\"/content/drive/MyDrive/Colab Notebooks/Dataset/new_data/test/image/*\"))\n",
        "    test_y = sorted(glob(\"/content/drive/MyDrive/Colab Notebooks/Dataset/new_data/test/mask/*\"))\n",
        "\n",
        "    \"\"\" Make the prediction and calculate the metrics values \"\"\"\n",
        "    SCORE = []\n",
        "    for x, y in tqdm(zip(test_x, test_y), total=len(test_x)):\n",
        "        \"\"\" Extracting name \"\"\"\n",
        "        name = x.split(\"/\")[-1].split(\".\")[0]\n",
        "\n",
        "        \"\"\" Read the image and mask \"\"\"\n",
        "        ori_x, x = read_image(x)\n",
        "        ori_y, y = read_mask(y)\n",
        "\n",
        "        \"\"\" Prediction \"\"\"\n",
        "        y_pred = model.predict(np.expand_dims(x, axis=0))[0]\n",
        "        y_pred = y_pred > 0.5\n",
        "        y_pred = y_pred.astype(np.int32)\n",
        "        y_pred = np.squeeze(y_pred, axis=-1)\n",
        "\n",
        "        \"\"\" Saving the images \"\"\"\n",
        "        save_image_path = f\"/content/drive/MyDrive/Colab Notebooks/Dataset/new_data/results2/{name}.png\"\n",
        "        save_results(ori_x, ori_y, y_pred, save_image_path)\n",
        "\n",
        "        \"\"\" Flatten the array \"\"\"\n",
        "        y = y.flatten()\n",
        "        y_pred = y_pred.flatten()\n",
        "\n",
        "        \"\"\" Calculate the metrics \"\"\"\n",
        "        acc_value = accuracy_score(y, y_pred)\n",
        "        f1_value = f1_score(y, y_pred,labels=[0, 1], average=\"macro\")\n",
        "        jac_value = jaccard_score(y, y_pred,labels=[0, 1], average=\"macro\")\n",
        "        recall_value = recall_score(y, y_pred,labels=[0, 1], average=\"macro\")\n",
        "        precision_value = precision_score(y, y_pred,labels=[0, 1], average=\"macro\")\n",
        "        SCORE.append([name, acc_value, f1_value, jac_value, recall_value, precision_value])\n",
        "\n",
        "    score = [s[1:] for s in SCORE]\n",
        "    score = np.mean(score, axis=0)\n",
        "    print()\n",
        "    print(f\"Accuracy: {score[0]:0.5f}\")\n",
        "    print(f\"F1: {score[1]:0.5f}\")\n",
        "    print(f\"Jaccard: {score[2]:0.5f}\")\n",
        "    print(f\"Recall: {score[3]:0.5f}\")\n",
        "    print(f\"Precision: {score[4]:0.5f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/Dataset/new_data/files/test\", \"wb\") as fp:   #Pickling\n",
        "   pickle.dump(average_weight, fp)\n",
        " "
      ],
      "metadata": {
        "id": "fYDOwntvQsPR"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/Colab Notebooks/Dataset/new_data/files/test\", \"rb\") as fp:   # Unpickling\n",
        "   loaded_weight = pickle.load(fp)\n",
        "   \n",
        "loaded_weight[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8F7iGOt28CQ",
        "outputId": "838667bf-895e-4a44-b9f6-e4c13084d45a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1.01460719e-04, -2.42991337e-05, -1.49040279e-04, -3.66890104e-04,\n",
              "        1.95960427e-04, -7.31397420e-04, -1.53886783e-03,  4.55071422e-05,\n",
              "        1.89139013e-04, -1.69523130e-03, -2.01566960e-03,  7.47707207e-04,\n",
              "       -1.22488977e-03,  3.83226579e-04, -6.90193614e-04,  4.90222737e-06,\n",
              "        7.76438508e-04, -3.35195422e-04,  2.38383841e-03,  7.91854691e-05,\n",
              "       -9.82870464e-04, -1.46525295e-03, -2.16422742e-03, -1.44217964e-04,\n",
              "        1.74445013e-05,  1.25694685e-04, -1.87646574e-03, -2.42949318e-05,\n",
              "        5.48933283e-04, -7.44943973e-04,  3.83559236e-04, -2.44313938e-04,\n",
              "       -2.13219566e-04, -5.50566110e-05,  6.83235296e-04, -1.03755519e-05,\n",
              "        7.87545519e-04,  2.93886464e-04,  2.59723834e-04,  1.21672812e-03,\n",
              "        1.18452472e-05, -1.70849147e-03,  7.54887646e-04,  7.33249355e-04,\n",
              "        1.49078405e-05,  5.39700632e-05,  1.36884977e-04,  9.03624314e-05,\n",
              "        4.03853017e-04, -1.33124471e-03, -3.08019924e-04, -5.34744759e-04,\n",
              "       -5.35444342e-05, -9.03141394e-04,  1.28424587e-03, -5.47469943e-04,\n",
              "       -1.60210638e-03,  1.91730098e-03, -1.91335363e-04,  6.51247508e-04,\n",
              "       -4.63198667e-05,  7.21847027e-05,  6.63992018e-04,  2.98505474e-04],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X3f7VsZb3fnh"
      },
      "execution_count": 32,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}